<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Variational-Inference on iamtu</title><link>https://iamtu.dev/tags/variational-inference/</link><description>Recent content in Variational-Inference on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 29 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://iamtu.dev/tags/variational-inference/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Variational Inference</title><link>https://iamtu.dev/posts/variational_inference/</link><pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/variational_inference/</guid><description>This post is a note I take from while reading Blei et al 2018.
Goal:
Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \(q(z)\) to approximate \(p(z|x)\)
The KL-divergence
$$ \begin{equation} \begin{aligned} KL[q(z)||p(z | x)] &amp;amp;= \int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz} \end{aligned} \end{equation} $$
However, this quantity is intractable to compute hence, we&amp;rsquo;re unable to optimize this quantity directly.</description></item></channel></rss>