[{"content":"This post mainly a\n","permalink":"http://localhost:1313/posts/adjoint_sensitivity_for_initial_value/","summary":"This post mainly a","title":"Adjoint Sensitivity Method for Inverse Problem"},{"content":" Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.\nThe entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand. So I hope this post might save sometime for people who are also trying to understand NeuralODE.\nNeuralODE Formulation Many systems can be written as a ODE\u0026rsquo;s initial value problem:\n$$ \\begin{equation} \\begin{cases} \\frac{d\\mathbf{u}}{dt} = f(\\mathbf{u}, t) \\\\ \\mathbf{u}(t=0) = \\mathbf{u}_0 \\end{cases} \\end{equation} $$\nWhere:\n\\(\\mathbf{u} \\in \\mathbb{R}^N\\) describes the state of the system. \\(\\mathbf{u}(t)\\) is the state of the system at time \\(t\\). \\(f: \\mathbb{R}^N \\rightarrow \\mathbb{R}^N\\) is a dynamic function that characterizes the system. Which means, at any given state of the system, \\(f\\) tells us the rate of change for that state \\(\\frac{d\\mathbf{u}}{dt}\\). When \\(f\\) is given, this system of equations can be solved by numerical methods such as Euler, Predictor Corrector Method, or RK45 (see my previous post on ODE Integrator).\nNeuralODE solves the inverse problem, given some observations of the system state \\(\\{(\\mathbf{u}_i, t_i)\\}_{i=1\\cdots N}\\) (which can be irregularly sampled), find the dynamic function \\(f\\). NeuralODE parameterize \\(f\\) by a neural-network \\(f_\\theta\\), where \\(\\theta \\in \\mathbb{R}^P\\) is the set of parameters of the network. For simplicity, assume that we have only two observations at \\(t=t_0\\) and \\(t=t_1\\). NeuralODE\u0026rsquo;s prediction of the system state at \\(t=t_1\\) are given by:\n$$ \\begin{equation} \\hat{\\mathbf{u}}_1 = \\mathbf{u}_0 + \\int_{t_0}^{t_1}{f(\\mathbf{u}, t;\\theta)dt} \\end{equation} $$\nNeuralODE learns the parameter \\(\\theta\\) by minimizing the difference between its prediction and groundtruth \\(\\mathbf{u}_1\\):\n$$ L(\\hat{\\mathbf{u}}_1, \\mathbf{u}_1) = \\text{MSE}(\\mathbf{u}_1, \\mathbf{\\hat{u}}_1) $$\nFinally, NeuralODE can be formulated as a constrained optimization problem:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\min_\\theta L(\\hat{\\mathbf{u}}_1, \\mathbf{u}_1) \\\\ \\text{such that: } \u0026amp; \\frac{d\\mathbf{u}}{dt} = f(\\mathbf{u}, t;\\theta)\\quad \\forall t_0\\leq t \\leq t_1 \\end{aligned} \\end{equation} $$\nIn order to solve the optimization problem in equation 3), we need to compute the sensitivity \\(\\frac{dL}{d\\theta}\\). The following sections will discuss the method of computing this quantity.\nThe forward sensitivity method The forward sensitivity method is a straight forward method of computing \\(\\frac{dL}{d\\theta}\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{dL}{d\\theta} \u0026amp; = \\frac{d}{d\\theta} L(\\mathbf{u}_1, \\mathbf{\\hat{u}}_1) \\\\ \u0026amp; = \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{\\hat{u}}_1}}_{1\\times N} \\underbrace{\\frac{d\\mathbf{\\hat{u}_1}}{d\\theta}}_{N\\times P} \\end{aligned} \\end{equation} $$\nBy differentiating both L.H.S and R.H.S of equation (1) with respect to \\(\\theta\\), we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) = \\frac{d}{d\\theta}f(\\mathbf{u}, t; \\theta) \\\\ \\iff \u0026amp; \\frac{d}{dt}(\\frac{d\\mathbf{u}}{d\\theta}) = \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial \\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} \\end{aligned} \\end{equation} $$\nEquation (5) gaves us a system of ODE initial value problem, which consists of \\(N\\times P\\) individual ODE. We can see that by denoting \\(A = \\frac{d\\mathbf{u}}{d\\theta} \\in \\mathbb{R}^N\\) is the Jacobian of state \\(u\\) with respect to parameter \\(\\theta\\), \\(A_{ij} = \\frac{\\partial u_i}{\\partial\\theta_j}\\). To solve for \\(\\frac{d\\mathbf{\\hat{u}_1}}{d\\theta}\\), we solve \\(N\\times P\\) individual ODE initial value problems with initial value is \\(0\\) (since \\(\\mathbf{u}_0\\) doesn\u0026rsquo;t depend on \\(\\theta\\)). The forward sensitivity method is computationally prohibited for the medium to large neural networks with thousands of parameters.\nEquiped with this understanding, we can fully appreciate the adjoint sensitivity method, which is the key to understand NeuralODE.\nThe adjoint state method We can write the constrainted optimization problem in equation (3) as unconstrainted one using continuous Lagrangian multiplier \\(\\lambda(t)\\):\n$$ \\begin{equation} \\begin{aligned} J(\\mathbf{\\hat{u}_1}, \\lambda, \\theta) = L(\\mathbf{\\hat{u}_1}) + \\int_{t_0}^{t_1}{\\lambda(t)(f - \\frac{d\\mathbf{u}}{dt} )dt} \\end{aligned} \\end{equation} $$\nWhere \\(\\lambda \\in \\mathbb{R}^N\\) is the Lagrangian multiplier in the form of function of time. Take the derivative with respect to \\(\\theta\\) for L.H.S and R.H.S in equation (6):\n$$ \\begin{equation} \\begin{aligned} \\frac{dJ}{d\\theta} \u0026amp; = \\frac{d}{d\\theta}\\bigg( L(\\mathbf{\\hat{u}_1}) + \\int_{t_0}^{t_1}{\\lambda(t)(f - \\frac{d\\mathbf{u}}{dt} )dt} \\bigg) \\\\ \u0026amp; = \\frac{d}{d\\theta} L(\\mathbf{\\hat{u}_1}) + \\int_{t_0}^{t_1}{ \\frac{d}{d\\theta} \\lambda(f - \\frac{d\\mathbf{u}}{dt}) dt} \\end{aligned} \\end{equation} $$\nConsidering the first term in R.H.S of equation (7):\n$$ \\begin{equation} \\begin{aligned} \\frac{d}{d\\theta}L(\\mathbf{\\hat{u}}_1) \u0026amp;= \\frac{\\partial L}{\\partial \\mathbf{\\hat{u}_1}} \\frac{d}{d\\theta}( \\mathbf{u}_0 + \\int_{t_0}^{t_1}{f(\\mathbf{u}, t; \\theta)dt}) \\\\ \u0026amp; = \\frac{\\partial L}{\\partial \\mathbf{\\hat{u}_1}} \\int_{t_0}^{t_1}{ \\bigg(\\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta}\\bigg)dt } \\end{aligned} \\end{equation} $$\nAnd considering the second term of the R.H.S of equation (7):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\int_{t_0}^{t_1}{ \\frac{d}{d\\theta}\\lambda \\bigg( f(\\mathbf{u}, t;\\theta) - \\frac{d\\mathbf{u}}{dt} \\bigg) dt }\\\\ = \u0026amp; \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} - \\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) \\bigg) dt }\\\\ = \u0026amp; \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} \\bigg) dt } - \\blue{ \\int_{t_0}^{t_1}{ \\lambda\\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) dt } } \\end{aligned} \\end{equation} $$\nThe final term in equation (9) can be evaluated by integration by parts after swapping the order of derivation:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\int_{t_0}^{t_1}{ \\lambda\\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) dt } \\\\ = \u0026amp; \\int_{t_0}^{t_1}{ \\lambda\\frac{d}{dt}(\\frac{d\\mathbf{u}}{d\\theta}) dt } \\\\ = \u0026amp; \\bigg[\\lambda \\frac{d\\mathbf{u}}{d\\theta}\\bigg]_{t_0}^{t_1} - \\int_{t_0}^{t_1}{\\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta}dt}\\\\ = \u0026amp; \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} - \\underbrace{\\cancel{ \\lambda(t_0)\\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_0} }}_{=0} - \\int_{t_0}^{t_1}{\\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta}dt}\\\\ \\end{aligned} \\end{equation} $$\nThe second term cancelled out due to state \\(u(t=0)\\) doesn\u0026rsquo;t depend on \\(\\theta\\). Replacing equation (10) back into equation (9):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\int_{t_0}^{t_1}{ \\frac{d}{d\\theta}\\lambda \\bigg( f(\\mathbf{u}, t;\\theta) - \\frac{d\\mathbf{u}}{dt} \\bigg) dt }\\\\ = \u0026amp; \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} + \\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta} \\bigg) dt } - \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} \\end{aligned} \\end{equation} $$\nReplacing result from equation (8) and (11) into equation (7):\n$$ \\begin{equation} \\begin{aligned} \\frac{dJ}{d\\theta} \u0026amp;= \\frac{\\partial L}{\\partial \\mathbf{\\hat{u}_1}} \\int_{t_0}^{t_1}{ \\bigg(\\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta}\\bigg)dt }\\\\ \u0026amp; + \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} + \\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta} \\bigg) dt }\\\\ \u0026amp; - \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} \\end{aligned} \\end{equation} $$\nRearranging equation (12):\n$$ \\begin{equation} \\begin{aligned} \\frac{dJ}{d\\theta} \u0026amp;= \\int_{t_0}^{t_1}{ \\big(\\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1} + \\lambda\\big)\\frac{\\partial f}{\\partial\\theta}dt }\\\\ \u0026amp; + \\int_{t_0}^{t_1}{ \\big( \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1}\\frac{\\partial f}{\\partial\\mathbf{u}} + \\lambda \\frac{\\partial f}{\\partial\\mathbf{u}} + \\frac{d\\lambda}{dt} \\big)\\frac{d\\mathbf{u}}{d\\theta} }\\\\ \u0026amp; - \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} \\end{aligned} \\end{equation} $$\nFrom the forward sensitivity method we know that \\(\\frac{d\\mathbf{u}}{d\\theta}\\) is prohibitively expensive, we can choose the Lagrangian \\(\\lambda\\) such that the last two terms in equation (13) vanish. Specifically:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\begin{cases} \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1}\\frac{\\partial f}{\\partial\\mathbf{u}} + \\lambda \\frac{\\partial f}{\\partial\\mathbf{u}} + \\frac{d\\lambda}{dt} = 0 \\\\ \\lambda(t_1) = 0 \\end{cases} \\\\ \\iff \u0026amp; \\begin{cases} \\frac{d\\lambda}{dt} = -\\big(\\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1} + \\lambda\\big) \\frac{\\partial f}{\\partial\\mathbf{u}}\\\\ \\lambda(t_1) = 0 \\end{cases} \\end{aligned} \\end{equation} $$\nDenoting \\(\\mathbf{a}(t)=\\lambda + \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}_1}}\\), equation (14) became:\n$$ \\begin{equation} \\begin{aligned} \\begin{cases} \\frac{d\\mathbf{a}}{dt} = -\\mathbf{a}(t)\\frac{\\partial f}{\\partial \\mathbf{u}}\\\\ \\mathbf{a}(t_1) = \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1} \\end{cases} \\end{aligned} \\end{equation} $$\nEquation (15) is a ODE terminal value problem, which can be solved by any ODE solver. The sensitivity \\(\\frac{dJ}{d\\theta}\\) in equation (13) became:\n$$ \\begin{equation} \\frac{dJ}{d\\theta} = \\int_{t_0}^{t_1}{\\mathbf{a}(t)\\frac{\\partial f}{\\partial \\theta}dt} \\end{equation} $$\n\\(\\mathbf{a}(t)\\) is exactly the adjoint state that mentioned in the original paper. In the paper, the authors went with alternative proof using Taylor Expansion.\nSummary strategy of computing the sensitivity \\(\\frac{dJ}{d\\theta}\\): In forward pass, \\(\\mathbf{\\hat{u}}_1 = \\text{ODESolve}(f_\\theta, \\mathbf{u}_0, t_0, t_1)\\), where dynamic is specified by neural-network \\(f_\\theta\\) Solve ODE terminal value problem specified by equation (15) for adjoint state \\(\\mathbf{a}(t)\\) Compute sensitivity \\(\\frac{dJ}{d\\theta}\\) Implementation Git to my version of implementation References Patric Kridge\u0026rsquo;s thesis On Neural Differential Equation. Efficient gradient computation for dynamical models ","permalink":"http://localhost:1313/posts/adjoint_state_method/","summary":"Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.\nThe entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand.","title":"Understanding NeuralODE"},{"content":"todo\nDerivation of second and forth order Runge-Kutta methods Comparison of truncation error with different step-size Ordinary Differential Equation (ODE) Initial Value Problem A differential equation is differential equation is a relationship between function \\(f(x)\\), its independent variable \\(x\\), and any number of its derivative. An ODE is a differential equation where the independent variable and its derivatives are in one dimension.\n$$ \\begin{equation} F(x, f(x), f^{(1)}(x), f^{(2)}, \\cdots f^{(n-1)}(x)) = f^{(n)}(x) \\end{equation} $$\nWhere \\(f^{(i)}\\) is the \\(i^{th}\\) order derivative of \\(f\\). Initial value is a set of known value at \\(x = 0\\), namely \\(f(0), f^{(1)}(0), f^{(2)}, \\cdots f^{(n-1)}(0)\\). Coupled with equation 1, the problem is known as the ODE Initial Value Problem.\nAn example is the dampen harmonic occillator with the setup as following diagram:\nThe motion of mass \\(m\\) is gorverned by set of equations:\n$$ \\begin{equation} \\begin{aligned} ODE \\quad \u0026amp; F(t, x, \\dot{x}) = -\\frac{c \\dot{(x)} + kx}{m} = \\ddot{x} \\\\ IV \\quad \u0026amp; x(0) = A\\\\ \u0026amp; \\dot{x}(0) = 0 \\end{aligned} \\end{equation} $$\nReduction or Order Denote \\(S(x)\\) to be a state of equation (1):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; S(x) = \\begin{bmatrix} f(x) \\\\ f^{(1)}(x) \\\\ \\vdots \\\\ f^{(n-1)}(x) \\\\ \\end{bmatrix} \\\\ \\end{aligned} \\end{equation} $$\nSo equation 1 can be rewriten as\n$$ \\begin{equation} \\begin{aligned} F(t, S(t)) = f^{(n)}(t) \\end{aligned} \\end{equation} $$\nTaking derivative of \\(S\\)\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{dS}{dt} = \\begin{bmatrix} f^{(1)}(x) \\\\ f^{(2)}(x) \\\\ \\vdots \\\\ f^{(n)}(x) \\\\ \\end{bmatrix} = \\begin{bmatrix} S_2(x) \\\\ S_3(x) \\\\ \\vdots \\\\ F\\big(x, S(t)) \\end{bmatrix} \u0026amp; \\text{(Equation 3)} \\\\ \u0026amp; = \\mathcal{F}(t, S(t)) \\end{aligned} \\end{equation} $$\nWhere \\(S_i(x)\\) is the \\(i^{th}\\) entry of \\(S(x)\\). The \\(n^{th}\\) order ODE is turned into \\(n\\) coupled ODEs, where \\(\\mathcal{F}\\) is a function that assemble the correct state vector.\nBack to the dampend harmonic occilliator, we can denote the state vector \\(S(t)\\) as:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; S(t) = \\begin{bmatrix} x(t)\\\\ \\dot{x}(t) \\end{bmatrix} \\\\ \\implies \u0026amp; \\frac{dS}{dt} = \\begin{bmatrix} \\dot{x}(t)\\\\ \\ddot{x}(t) \\end{bmatrix} \\\\ \u0026amp; = \\begin{bmatrix} S_2(t)\\\\ F(t, x, \\dot{x}) \\end{bmatrix} \u0026amp; \\text{(ODE in 2)} \\\\ \u0026amp; = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ -k/m \u0026amp; -c/m \\end{bmatrix} \\begin{bmatrix} x(t)\\\\ \\dot{x}(t) \\end{bmatrix} \\\\ \u0026amp; = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ -k/m \u0026amp; -c/m \\end{bmatrix}S(t) \\end{aligned} \\end{equation} $$\nSo the second order ODE describes motion of mass \\(m\\) is transformed into a first order ODE of the state \\(S(t)\\).\nNumerical methods of solving first order ODE Given the formulation \\(\\frac{dS}{dt} = \\mathcal{F}(t, S(t))\\), and a regular grid on temporal interval \\([0, T]: \\{t_0, t_1,\\cdots t_N\\}\\), where \\(t_i = i\\frac{T}{N}=:ih\\). The Taylor expansion of \\(S\\) about \\(t\\) is given by:\n$$ \\begin{equation} \\begin{aligned} S(t+h) = S(t) + \\sum_1^k{ \\frac{h^k}{k!} \\frac{d^{(k)}S}{dt}(t) } + \\mathcal{O}(h^{k+1}) \\end{aligned} \\end{equation} $$\nEuler method The Euler method approximate the next state by simply truncating the Taylor expansion after the first derivative.\n$$ \\begin{equation} \\begin{aligned} \\hat{S}(t_{i+1}) = S(t_i + h) \u0026amp; = S(t_i) + h \\frac{dS}{dt}(t_i) + \\mathcal{O}(h^2)\\\\ \u0026amp; = S(t_i) + h\\mathcal{F}(t_i, S(t_i)) + \\underbrace{\\red{\\mathcal{O}(h^2)}}_{\\text{Truncation Error}} \\end{aligned} \\end{equation} $$\nGiven \\(\\mathcal{F}, S_0\\) we can sequentially compute \\(S\\) at any time \\(t\\).\n(code) JAX Implementation of Euler method euler() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def euler(z0: jnp.ndarray, t0: float, t1: float, f: callable, return_seq: bool = False): n_steps = int(jnp.ceil(jnp.abs(t1 - t0)/H_MAX)) # Compute step size h = (t1 - t0)/n_steps t = t0 z = z0 # sequence of z seq = [(z, t)] for i in range(n_steps): z = z + h * f(z, t) t = t + h seq.append((z, t)) if return_seq: return z, seq return z Let\u0026rsquo;s try this on the dampen harmonic occilliator example with parameters:\n1 2 3 4 5 6 7 c: .1 # Dampener coef k: 1. # Spring coef m: 1. # mass A: 1. # Initial position V: -1. # Inital velocity t0: .0 # Start time t1: 100. # Terminal time (code) dampen_harmonic_occiliator() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # Dampen Harmoic Occililator def dampen_harmonic_occiliator( c : PositiveFloat, k : PositiveFloat, m : PositiveFloat, A : float, V : float, t0 : float, t1 : float): \u0026#34;\u0026#34;\u0026#34; Problem: ODE: x\u0026#39;\u0026#39; + c/m x\u0026#39; + k/m x = 0 IV: x(t0) = A, x\u0026#39;(t0) = V Params: c: dampener coefficient k: spring coefficient m: mass A, V: initial position and velocity t0, t1: start and terminal timestamp \u0026#34;\u0026#34;\u0026#34; # constructing dynamic function F = lambda S, t: jnp.array([[0, 1],[-k/m, -c/m]]) @ S # initial condition S = jnp.array([A, V]) # Terminal S and trajectory of S S_t, Tr = euler(S, t0, t1, F, True) return S_t, Tr 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Terminal state \u0026amp; Trajectory of S S_T, Tr = dampen_harmonic_occiliator( config.c, config.k, config.m, config.A, config.V, config.t0, config.t1,) # Position X = [i[0][0] for i in Tr] T = [i[1] for i in Tr] plt.plot(T, X) plt.show() Results Runge-Kutta Second-Order Runge-Kutta Method Second-Order Runge-Kutta Method reduces the truncation errors in the estmation of the next state to \\(\\mathcal{O}(h^3)\\) by keeping the \\(3^{rd}\\) term in the Taylor expansion.\n$$ \\begin{equation} S(t_{i+1}) = S(t_i + h) = S(t_i) + h \\mathcal{F}(t_i, S(t_i)) + \\underbrace{ \\frac{h^2}{2!} \\mathcal{F}^\\prime(t_i, S(t_i)) }_{A} + \\mathcal{O}(h^3) \\end{equation} $$\nConsider quantity \\(A\\):\n$$ \\begin{equation} \\begin{aligned} A \u0026amp; = \\frac{d}{dt} \\mathcal{F}(t, S(t)) = \\frac{\\partial\\mathcal{F}}{\\partial t} + \\frac{\\partial F}{\\partial S}\\frac{dS}{dt} \\\\ \u0026amp; = \\frac{\\partial\\mathcal{F}}{\\partial t} + \\frac{\\partial\\mathcal{F}}{\\partial S} F \u0026amp; \\text{(Equation 5)} \\end{aligned} \\end{equation} $$\nSubstitute equation 10 into equation 9:\n$$ \\begin{equation} \\begin{aligned} S(t_{i+1}) = S(t_i + h) \u0026amp; = S + h\\mathcal{F} \\\\ \u0026amp; + \\frac{h^2}{2}\\big( \\frac{\\partial\\mathcal{F}}{\\partial t} + \\frac{\\partial\\mathcal{F}}{\\partial S}\\mathcal{F} \\big) \\\\ \u0026amp;+ \\mathcal{O}(h^3) \\end{aligned} \\end{equation} $$\nConsider approximation of \\(\\mathcal{F}\\) in the neighbor of \\((t, S\\)\n$$ \\begin{equation} \\begin{aligned} \\mathcal{F}(t + ph, S + qhF) \u0026amp; = F + \\frac{\\partial\\mathcal{F}}{\\partial t}\\delta t + \\frac{\\partial\\mathcal{F}}{\\partial S} \\delta S \\\\ \u0026amp; = S + \\end{aligned} \\end{equation} $$\n(code) JAX Implementation of Second-Order Runge Kutta method RK2() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def RK2(z0, t0, t1, f, return_seq: bool): \u0026#34;\u0026#34;\u0026#34; Second-Order Runge-Kutta Method S(t + h) = S(t) + 1/2(k1 + k2)h k1 = F(t, S(t)) k2 = F(t + h, S(t) + hk1) \u0026#34;\u0026#34;\u0026#34; n_steps = int(jnp.ceil(jnp.abs(t1 - t0)/H_MAX)) # Compute step size h = (t1 - t0)/n_steps t = t0 z = z0 seq = [(z, t)] for i in range(n_steps): k1 = f(z, t) k2 = f(z + h * k1, t + h) z = z + .5 * (k1 + k2) * h t = t + h seq.append((z, t)) if return_seq: return z, seq return z Fourth-Order Runge-Kutta Method $$ \\begin{equation} \\begin{aligned} \\hat{S}(t + h) \u0026amp; = S(t) + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4) + \\underbrace{\\red{\\mathcal{O}(h^4)}}_{\\text{Truncation error}} \\\\ \\text{Where:}\\\\ k_1 \u0026amp; = \\mathcal{F}(t, S(t)) \\\\ k_2 \u0026amp; = \\mathcal{F}(t + \\frac{h}{2}, S(t) + \\frac{hk_1}{2}) \\\\ k_3 \u0026amp; = \\mathcal{F}(t + \\frac{h}{2}, S(t) + \\frac{hk_2}{2} \\\\ k_4 \u0026amp; = \\mathcal{F}(t + h, S(t) + hk_3) \\\\ \\end{aligned} \\end{equation} $$\n(code) JAX Implementation of Second-Order Runge Kutta method RK2() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def RK4(z0, t0, t1, f, return_seq: bool): \u0026#34;\u0026#34;\u0026#34; Fourth-Order Runge-Kutta Method S(t + h) = S(t) + 1/2(k1 + k2)h k1 = F(t, S(t)) k2 = F(t + .5h, S(t) + .5 h k1) k3 = F(t + .5h, S(t) + .5 h k2) k4 = F(t + h, S(t) + h k3) \u0026#34;\u0026#34;\u0026#34; n_steps = int(jnp.ceil(jnp.abs(t1 - t0)/H_MAX)) # Compute step size h = (t1 - t0)/n_steps t = t0 z = z0 seq = [(z, t)] for i in range(n_steps): k1 = f(z, t) k2 = f(z + .5 * k1 * h, t + .5 * h) k3 = f(z + .5 * k2 * h, t + .5 * h) k4 = f(z + k3 * h, t + h) z = z + (k1 + 2 * k2 + 2 * k3 + k4) * h / 6 seq.append((z, t)) if return_seq: return z, seq return z Empirical result Numerical error by different step-size Position and Trajectory in state-space References (Book) Partial Differential Equations for Scientists and Engineers - Standley J. Farlow (Book) Python Programming and Numerical Methods - A Guide - Chapter 22 (Web) Introduction to Taylor\u0026rsquo;s theorem for multivariable functions (Web) Analytical solution to Damped Harmonic Occiliator - https://phys.libretexts.org ","permalink":"http://localhost:1313/posts/ode_solver/","summary":"todo\nDerivation of second and forth order Runge-Kutta methods Comparison of truncation error with different step-size Ordinary Differential Equation (ODE) Initial Value Problem A differential equation is differential equation is a relationship between function \\(f(x)\\), its independent variable \\(x\\), and any number of its derivative. An ODE is a differential equation where the independent variable and its derivatives are in one dimension.\n$$ \\begin{equation} F(x, f(x), f^{(1)}(x), f^{(2)}, \\cdots f^{(n-1)}(x)) = f^{(n)}(x) \\end{equation} $$","title":"Numerical Integrations"},{"content":" My goal for this post is to have a basic understanding of Calculus of Variations, so that I can be more comfortable with mathematics in NeuralODE paper, where the problem can be formulated as a optimization of a functional with ODE constraint (Adjoint State Method for an ODE).\nMy first encounter with Calculus of Variation is one of my homework where we try to derive probablity density function of some distribution by the principle of maximum entropy. This is my note of a more thorough investigation of the topic and it is heavily based on the content of this tutorial.\nSimilar to differential calculus where we try to find a stationary point, calculus of variations find a function to either minimize or maximize a functional (which is a function that take another function as argument).\nMotivating examples Shortest path between two points A path \\(y(x)\\) that passes two points \\(P_A(a, A), P_B(b, B)\\), so that \\(y(a)=A\\) and \\(y(b)=B\\).\nConsider an infinitestimal segment along the path \\(y\\), the length of this curve is given by:\n$$ \\begin{equation} dS = \\sqrt{dx^2 + dy^2} = \\sqrt{1 + (\\dot{y})^2} dx \\end{equation} $$\nThen the length of curve \\(y\\) is given by the sum of all \\(dS\\) along \\(y\\):\n$$ \\begin{equation} S[y] = \\int_{P_A}^{P_B}{dS} = \\int_a^b{\\sqrt{1 + (\\dot{y})^2}dx} \\end{equation} $$\n\\(S\\) is called a length functional of \\(y\\). The goal is to find function \\(y\\) to minimize \\(S\\).\nBrachistochrone curve A more exciting example would be the Brachistochrone problem, which was posed by Johann Bernoulli in 1696:\nGiven two points A and B in a vertical plane, what is the curve traced out by a point acted on only by gravity, which starts at A and reaches B in the shortest time.\nThe apparatus can be depicted in following diagram. In order to simplify the problem, we let two points \\(A, B\\) at \\(A(0, 0)\\) and \\(B(b, 1)\\) respectively.\nThe total energy of this system can be described as\n$$ \\begin{equation} E = \\frac{1}{2}mv^2 - mgy \\end{equation} $$\nWhere \\(m, v\\) is the mass and velocity of the point respectively. At \\(t = 0\\), both \\(v, y\\) are \\(0\\), so that the total engery of the system is 0. Due to the law of conservation of enery:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; E = \\frac{1}{2}mv^2 - mgy = 0 \\\\ \\implies \u0026amp; v = \\sqrt{2gy} \\end{aligned} \\end{equation} $$\nLet \\(S\\) be the length of the curved traced out by the point, by definition of velocity we have\n$$ \\begin{equation} v = \\frac{dS}{dt} = \\sqrt{2gy} \\end{equation} $$\nFor an infinitestimal segment of the curve \\(y\\), we have\n$$ \\begin{equation} dS = \\sqrt{dx^2 + dy^2} = \\sqrt{1 + (\\dot{y})^2}dx \\end{equation} $$\nSubstituing eq (7) into eq (6), we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; v = \\frac{\\sqrt{1+(\\dot{y})^2}dx}{dt} = \\sqrt{2gy} \\\\ \\implies \u0026amp; dt =\\sqrt{\\frac{1+(\\dot{y})^2}{2gy}}dx \\\\ \\implies \u0026amp; T[y] = \\int_0^b{\\sqrt{\\frac{1+(\\dot{y})^2}{2gy}}dx} \\end{aligned} \\end{equation} $$\n\\(T\\) is the time functional of function \\(y\\), the goal is to find y such that \\(T\\) is minimized.\nEuler-Lagrange equation Let \\(S\\) be a functional of some function \\(y(x)\\) that go through two points \\(P_a(a, A)\\) and \\(P_b(b, B)\\), defined by\n$$ \\begin{equation} \\begin{aligned} S[y] = \\int_a^b {F(x, y, \\dot{y}) dx} \\quad y(a)=A, y(b) = B \\end{aligned} \\end{equation} $$\nWhere is \\(F\\) is some function of \\(x, y\\), and \\(\\dot{y}\\). The Euler-Lagrange equation states that in order for \\(y\\) to be a stationary path of the functional \\(S\\), the following equality must hold:\n$$ \\begin{equation} \\frac{\\partial F}{\\partial y} - \\frac{d}{dx}\\bigg(\\frac{\\partial F}{\\partial \\dot{y}}\\bigg) = 0 \\end{equation} $$\nDerivation of Euler-Lagrange equation Let \\(\\tilde{y}(x) = y + \\epsilon \\eta(x)\\), such that \\(\\eta(a) = \\eta(b) = 0 \\) be some function that passes through \\(P_a, P_b\\). For \\(y\\) to be a stationary path of \\(S\\):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{d}{d\\epsilon}S[\\tilde{y}] \\bigg\\vert_{\\epsilon=0} = 0 \\\\ \\iff \u0026amp; \\frac{d}{d\\epsilon}\\int_a^b{F(x, \\tilde{y}, \\dot{\\tilde{y}}) dx} = 0 \\\\ \\iff \u0026amp; \\int_a^b{ \\frac{d}{d\\epsilon}F(x, \\tilde{y}, \\dot{\\tilde{y}}) \\big\\vert_{\\epsilon=0} dx } = 0 \\\\ \\iff \u0026amp; \\int_a^b{ \\big[ \\underbrace{ \\frac{\\partial F}{\\partial x} \\frac{dx}{d\\epsilon} }_{=0} + \\frac{\\partial F}{\\partial \\tilde{y}} \\frac{d\\tilde{y}}{d\\epsilon} + \\frac{\\partial F}{\\partial \\dot{\\tilde{y}}} \\frac{d\\dot{\\tilde{y}}}{d\\epsilon} \\big] \\bigg\\vert_{\\epsilon=0} dx} = 0 \\\\ \\iff \u0026amp; \\int_a^b{\\bigg[ \\frac{\\partial F}{\\partial \\tilde{y}} \\eta + \\frac{\\partial F}{\\partial \\dot{\\tilde{y}}} \\dot{\\eta} \\bigg]\\bigg\\vert_{\\epsilon=0}dx} = 0 \\end{aligned} \\end{equation} $$\nWhen \\(\\epsilon\\rightarrow 0 : \\tilde{y} \\rightarrow y \\), subsitute into equation 3:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\int_a^b{\\bigg( \\frac{\\partial F}{\\partial y} \\eta + \\frac{\\partial F}{\\partial \\dot{y}} \\dot{\\eta} \\bigg)dx} = 0 \\\\ \\iff \u0026amp; \\int_a^b{\\frac{\\partial F}{\\partial y} \\eta dx} + \\underbrace{\\bigg[\\frac{\\partial F}{\\partial\\dot{y}}\\eta\\bigg]_a^b}_{=0} - \\int_a^b{\\frac{d}{dx}\\big(\\frac{\\partial F}{\\partial\\dot{y}}\\big) dx} = 0 \u0026amp; \\quad \\text{(Integration by part)} \\\\ \\iff \u0026amp; \\int_a^b{\\bigg( \\frac{\\partial F}{\\partial y} - \\frac{d}{dx} \\big(\\frac{\\partial F}{\\partial \\dot{y}}\\big) \\bigg)\\eta dx} = 0 \\end{aligned} \\end{equation} $$\nBecause \\(\\eta\\) is arbitrary, in order for equality (4) to hold, quantity \\( \\frac{\\partial F}{\\partial y} - \\frac{d}{dx}\\big(\\frac{\\partial F}{\\partial \\dot{}y}\\big) \\) has to be equal to \\(0\\).\n\\begin{equation} \\blue{ \\frac{\\partial F}{\\partial y} - \\frac{d}{dx}\\big(\\frac{\\partial F}{\\partial \\dot{y}}\\big) = 0 } \\end{equation}\nThis equation is known as Euler-Lagrange equation. Any function \\(y\\) satisfied this equation will be a stationary path of functional \\(S\\).\nAlternative forms of the Euler-Lagrange equation Special case where \\(F\\) does not explicitly depend on \\(y\\) \\(F(x, \\dot{y})\\) Because \\(F\\) does not explicity dependence on \\(y\\), we have \\(\\partial F/\\partial y =0\\). By equation 12, it immediately follows:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{d}{dx}(\\frac{\\partial F}{\\partial \\dot{y}}) = 0 \\\\ \\iff \u0026amp; \\frac{\\partial F}{\\partial \\dot{y}} = \\text{constant} \\end{aligned} \\end{equation} $$\nFirst integral of Euler-Lagrange equation \\(F(y, \\dot{y})\\) Consider the total derivative of \\(F\\) with respect to \\(x\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{dF}{dx} \u0026amp; = \\underbrace{\\frac{\\partial F}{\\partial x}}_{=0} + \\frac{\\partial F}{\\partial y}\\frac{dy}{dx} + \\frac{\\partial F}{\\partial \\dot{y}}\\frac{d \\dot{y}}{dx} \u0026amp; (F \\text{ does not depend on }x) \\\\ \u0026amp; = \\frac{d}{dx}\\big(\\frac{\\partial F}{\\partial \\dot{y}}\\big)\\dot{y} + \\frac{\\partial F}{\\partial \\dot{y}}\\ddot{y} \u0026amp; \\text{(Equation 12)}\\\\ \u0026amp; = \\frac{d}{dx}\\big(\\dot{y}\\frac{\\partial F}{\\partial\\dot{y}}\\big) \u0026amp; u\\dot{v} + v\\dot{u} = \\dot{(uv)} \\\\ \\implies \u0026amp; \\dot{y}\\frac{\\partial F}{\\partial\\dot{y}} - F = \\text{constant} \\end{aligned} \\end{equation} $$\nThis equation is called the first integral of Euler-Lagrange equation.\nApplying Euler-Lagrange equation to the motivating examples Shortest path between two point on a plane Reiterate that the path length of curve \\(y(x)\\) is a functional \\(S\\) of \\(y\\):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; S[y] = \\int_{P_A}^{P_B}{dS} = \\int_a^b{\\sqrt{1 + (\\dot{y})^2}dx} =: \\int_a^b{F(\\dot{y})dx};\\\\ \u0026amp; y(a)=A, y(b) = B \\end{aligned} \\end{equation} $$\nFrom equation 15, we have\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{\\partial F}{\\partial \\dot{y}} = \\text{constant} \\\\ \\iff \u0026amp; \\frac{\\partial}{\\partial \\dot{y}}\\sqrt{1 + (\\dot{y})^2} = \\text{constant} \\\\ \\iff \u0026amp; \\frac{\\dot{y}}{\\sqrt{1 + (\\dot{y})^2}} = \\text{constant} \\\\ \\iff \u0026amp; \\dot{y} = \\text{constant} \\\\ \\implies \u0026amp; y = ax + b \\end{aligned} \\end{equation} $$\nSo the shortest path between two points on a plane is a line.\nBrachistochrone problem Time functional \\(T\\) of curve \\(y(x)\\):\n$$ \\begin{equation} \\begin{aligned} T[y] = \\int_0^b{\\sqrt{\\frac{1+(\\dot{y})^2}{2gy}}dx} =: \\int_0^b{F(y,\\dot{y}) dx} \\end{aligned} \\end{equation} $$\nUsing the first integral of the Euler-Lagrange equation, we have\n\\begin{equation} \\begin{aligned} \u0026amp; \\dot{y}\\frac{\\partial F}{\\partial \\dot{y}} - F = c \u0026amp; \\text{(c is some constant)} \\\\ \\iff \u0026amp; \\dot{y}\\frac{\\partial}{\\partial{\\dot{y}}}\\big( \\sqrt{\\frac{1 + (\\dot{y})^2}{2gy}} \\big) - \\sqrt{\\frac{1 + (\\dot{y})^2}{2gy}} = c \\\\ \\iff \u0026amp; \\frac{(\\dot{y})^2}{\\sqrt{2gy[1+(\\dot{y})^2]}} - \\sqrt{\\frac{1 + (\\dot{y})^2}{2gy}} = c \\\\ \\iff \u0026amp; \\frac{1}{\\sqrt{y[1+(\\dot{y})^2]}} = k \u0026amp; \\text{(for some constant k)} \\\\ \\implies \u0026amp; \\dot{y} = \\sqrt{\\frac{h^2}{y} - 1} = \\sqrt{\\frac{h^2 - y}{y}} \u0026amp; (\\text{for constant } h^2=1/k^2 \u0026gt; 0) \\\\ \\implies \u0026amp; dx = \\sqrt{\\frac{y}{h^2 - y}}dy \\end{aligned} \\end{equation}\nLet \\(y = h^2 \\sin^2\\theta\\), so \\(dy = 2h^2\\sin\\theta\\cos\\theta d\\theta\\). Subsititutes this into equation 18, we have\n$$ \\begin{equation} \\begin{aligned} dx \u0026amp; = \\sqrt{\\frac{h^2 \\sin^2\\theta}{h^2(1 - \\sin^2\\theta)}}2h^2\\sin\\theta\\cos\\theta d\\theta \\\\ \u0026amp; = 2h^2 \\sin^2\\theta d\\theta \\end{aligned} \\end{equation} $$\nTake the antiderivative for both sides, we have\n$$ x = \\frac{h^2}{2}(2\\theta - \\sin 2\\theta) + c $$\nFinally, we have \\(x = \\frac{h^2}{2}(2\\theta - \\sin 2\\theta) + c\\) and \\(y = h^2\\sin^2\\theta\\), for some \\(h, c\\) determined by \\(y(0) = 0, y(b) = 1\\). In the tutorial, \\(h\\) and \\( c\\) can only be determined numerically.\nReferences Introduction to Calculus of Vartiations - Open University Adjoint State Method for an ODE Original NeuralODE paper ","permalink":"http://localhost:1313/posts/variational_calculus/","summary":"My goal for this post is to have a basic understanding of Calculus of Variations, so that I can be more comfortable with mathematics in NeuralODE paper, where the problem can be formulated as a optimization of a functional with ODE constraint (Adjoint State Method for an ODE).\nMy first encounter with Calculus of Variation is one of my homework where we try to derive probablity density function of some distribution by the principle of maximum entropy.","title":"My calculus of variations crash course"},{"content":" Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:\n(Book) Partial Differential Equations for Scientists and Engineers - Standley J. Farlow for deriving closed-form solution. (Article) Finite-Difference Approximations to the Heat Equation (Course) ETH Zurich | Deep Learning for Scientific Computing 2023 for Theory and Implementation of Physics-Informed Neural Network. Introduction Physics-Informed Machine Learning (PIML) is an exciting subfield of Machine Learning that aims to incorporate physical laws and/or constraints into statistical machine learning. The representations of the laws and constraints can be categorized into three groups (with decreasing strength of inductive bias):\nPartial differential equations (PDE) Symmetry: translation, rotation invariant. And intuitive physical constraints. The PINN method incorporates PDE into the learning problem by adding PDE as a regularization term into the machine learning loss term.\nHeat equations This instance of the 1D heat equation describes how the temperature of an insulated rod changes over time (transient state) at any point on the rod, where the two ends of the rod are kept at a constant temperature of \\(0^o C\\) and the initial temperature of the rod was given by a function of location \\(x\\).\n$$ \\begin{equation} \\begin{aligned} PDE: \u0026amp; \u0026amp; u_t = \\alpha^2 u_{xx} \u0026amp; \u0026amp; 0 \u0026lt; x\u0026lt; 1 \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ BCs: \u0026amp; \u0026amp; \\begin{cases} u(0, t) = 0\\\\ u(1, t) = 0 \\end{cases} \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ IC: \u0026amp; \u0026amp; u(x, 0) = \\sin(2\\pi x) \u0026amp; \u0026amp; 0 \\leq x \\leq 1 \\end{aligned} \\end{equation} $$\nSolving heat equation with variables seperation Suppose that we can factorize \\(u(x, t) = X(x)T(t)\\), from the PDE we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; X(x)T^\\prime(t) = \\alpha^2 X^{\\prime\\prime}(x)T(t)\\\\ \\implies \u0026amp; \\frac{T^\\prime(t)}{\\alpha^2 T(t)} = \\frac{X^{\\prime\\prime}(x)}{X(x)} = \\mu \\\\ \\implies \u0026amp; \\begin{cases} T^\\prime(t) - \\mu\\alpha^2 T(t) = 0 \u0026amp; \u0026amp; (2a) \\\\ X^{\\prime\\prime}(x) - \\mu X(x) = 0 \u0026amp; \u0026amp; (2b) \\end{cases} \\end{aligned} \\end{equation} $$\nFrom equation (2a), \\(T(t) = Ae^{\\mu\\alpha^2t}\\). This implies \\(\\mu\\) must be negative so that \\(T\\) doesn\u0026rsquo;t go to \\(\\infty\\). Let \\(\\mu = -\\lambda^2\\), so \\(T(t) = Ae^{-\\lambda^2\\alpha^2t}\\). Replacing into (2), we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; X^{\\prime\\prime}(x) + \\lambda^2 X(x) = 0 \\\\ \\implies \u0026amp; X(x) = B \\sin\\lambda x + C\\cos\\lambda x \\end{aligned} \\end{equation} $$\nSubstitute \\(T(t), X(x)\\) into \\(u(x, t)\\):\n$$ \\begin{equation} u(x, t) = e^{-\\lambda^2\\alpha^2 t}(A\\sin\\lambda x + B\\cos\\lambda x) \\end{equation} $$\nSubsititute this into boundary conditions:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\begin{cases} u(0, t) = 0 \\\\ u(1, t) = 0 \\end{cases} \\\\ \\implies \u0026amp; \\begin{cases} e^{-\\lambda^2\\alpha^2 t}(A \\sin 0 + B \\cos 0)= 0 \\\\ e^{-\\lambda^2\\alpha^2 t}(A \\sin \\lambda + B \\cos \\lambda) = 0 \\\\ \\end{cases}\\\\ \\implies \u0026amp; \\begin{cases} B = 0 \\\\ \\lambda = n\\pi \u0026amp; n = 1, 2, \\cdots \\end{cases} \\end{aligned} \\end{equation} $$\nSo for a given \\(n\\), we have a particular solution for \\(u(x, t)\\):\n$$ \\begin{equation} u_n(x, t) = A_n e^{-n^2\\pi^2\\alpha^2 t} \\sin n\\pi x \\end{equation} $$\nAnd the general solution for \\(u(x, t)\\):\n$$ \\begin{equation} u(x, t) = \\sum_{n=1}^\\infty{A}_n e^{-n^2\\pi^2\\alpha^2t} \\sin n\\pi x \\end{equation} $$\nWhere \\(A_n\\) is given by:\n$$ \\begin{equation} A_n = 2\\int_0^1 \\sin 2\\pi x\\sin n\\pi x dx = \\begin{cases} 0 \\quad \\text{if }n \\neq 2\\\\ 1 \\quad \\text{if }n = 2 \\end{cases} \\end{equation} $$\nFinally, we have the solution to the PDE:\n$$ \\begin{equation} \\blue{u(x, t) = e^{-4\\pi^2\\alpha^2t}\\sin 2\\pi x} \\end{equation} $$\nFinite Difference Method Numerical approximation of first and second order derivative First Order Forward Difference Consider a Taylor series expansion of \\(\\phi(x)\\) about point \\(x_i\\):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\delta x) = \\phi(x_i) + \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\delta x^2}{2!} + \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\delta x^3}{3!} + \\cdots \\\\ \\end{aligned} \\end{equation} $$\nReplace \\(\\delta x = \\Delta x \\ll 1\\) in equation (10):\n$$ \\begin{aligned} \u0026amp; \\phi(x_i + \\delta x) = \\phi(x_i) + \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} + \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\\\ \\implies \u0026amp; \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} = \\frac{\\phi(x_i +\\Delta x) - \\phi(x_i)}{\\Delta x} \\red{\\underbrace{ - \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{3!} - \\cdots }_{\\text{Truncation error: }\\mathcal{O}(\\Delta x)}}\\\\ \u0026amp; \\blue{\\approx \\frac{\\phi(x_i +\\Delta x) - \\phi(x_i)}{\\Delta x}} \\end{aligned} $$\nNote that in this tutorial, the truncation error is \\(\\mathcal{O}(\\Delta x^2)\\). I haven\u0026rsquo;t been able to understand why yet!!!.\nFirst Order Backward Difference Replace \\(\\delta x = -\\Delta x, \\Delta x \\ll 1\\) in equation (10):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\delta x) = \\phi(x_i) - \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\\\ \\implies \u0026amp; \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} = \\frac{\\phi(x_i) - \\phi(x_i - \\Delta x)}{\\Delta x} \\red{\\underbrace{ + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{3!} + \\cdots }_{\\text{Truncation error: }\\mathcal{O}(\\Delta x)}}\\\\ \u0026amp; \\blue{\\approx \\frac{\\phi(x_i) - \\phi(x_i - \\Delta x)}{\\Delta x}} \\end{aligned} \\end{equation} $$\nSecond Order Central Difference Replace in equation (10):\n\\(\\delta x = \\Delta x\\) $$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\Delta x) = \\phi(x_i) + \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} + \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\end{aligned} \\end{equation} $$\n\\(\\delta x = -\\Delta x\\) $$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i - \\Delta x) = \\phi(x_i) - \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\end{aligned} \\end{equation} $$\nAdding equation (12) and (13) we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\Delta x) + \\phi(x_i - \\Delta x) = 2 \\phi(x_i) + 2 \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} + 2 \\frac{\\partial^4 \\phi}{\\partial x^4}\\bigg\\vert_{x_i} \\frac{\\Delta x^4}{4!} + \\cdots\\\\ \\implies \u0026amp; \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} = \\frac{\\phi(x_i + \\Delta x) - 2\\phi(x_i) + \\phi(x - \\Delta x)}{\\Delta x^2} \\red{\\underbrace{ - 2 \\frac{\\partial^4 \\phi}{\\partial x^4}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{4!} - \\cdots}_{\\mathcal{O}(\\Delta x^2)}}\\\\ \u0026amp; \\blue{ \\approx \\frac{\\phi(x_i + \\Delta x) - 2\\phi(x_i) + \\phi(x - \\Delta x)}{\\Delta x^2} } \\end{aligned} \\end{equation} $$\nFinite Difference Method for the Heat Equation Discretize the domain \\(\\mathcal{D} = (0, 1) \\times (0, T)\\) by constructing a grid \\(\\{x_i\\}_{i=1\\cdots N} \\times \\{t_m\\}_{m=1\\cdots M}\\). Where:\n\\(x_i = (i - 1) \\Delta x,\\quad \\Delta x = \\frac{1}{N - 1}\\) \\(t_m = (m - 1) \\Delta t,\\quad \\Delta t = \\frac{T}{M - 1}\\) Let \\(u(x, t)\\) be the true solution to the PDE\nForward Time, Centered Space (FTCS) Using First Order Forward Difference (equation 10) to approximate parital derivative of \\(u\\) at a grid point \\((x_i, t_m)\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{\\partial u}{\\partial t} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i, t_m + \\Delta t) - u(x_i, t_m)}{\\Delta t} + \\mathcal{O}(\\Delta t)\\\\ \u0026amp; \\approx \\frac{u_i^{m+1}-u_i^m}{\\Delta t} \\end{aligned} \\end{equation} $$\nUsing Second Order Central Difference (equation 14) to approximate the second order partial derivative of \\(u\\) with respect to \\(x\\) at the grid point: $$ \\begin{equation} \\begin{aligned} \\frac{\\partial^2 u}{\\partial x^2} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i + \\Delta x, t_m) - 2 u(x_i, t_m) + u(x_i - \\Delta x, t_m)}{\\Delta x^2} + \\mathcal{O}(\\Delta x^2)\\\\ \u0026amp; \\approx \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2} \\end{aligned} \\end{equation} $$\nWhere \\(u_i^m\\) is the numerical approximation of true function evaluated at the grid point \\((x_i, t_m)\\). Replacing equation (15) and (16) into the LHS and RHS of the PDE in (1):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{u_i^{m+1}-u_i^m}{\\Delta t} = \\alpha^2 \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2}\\\\ \\implies \u0026amp; u_i^{m+1} = u_i^m + \\frac{\\alpha^2 \\Delta t}{\\Delta x^2} (u_{i+1}^m - 2 u_i^m + u_{i-1}^m) \\\\ \u0026amp; = \\blue{u_i^m(1 - 2r) + r(u_{i+1}^m + u_{i-1}^m)} \\end{aligned} \\end{equation} $$\nWhere \\(r = \\frac{\\alpha^2\\Delta t}{\\Delta x^2}\\). In order for \\(u(x, t)\\) reach steady state, \\(r\\) must be smaller than \\(\\frac{1}{2}\\). The proof was provided in Von Neumann Stability Analysis.\nTODO: Haven\u0026rsquo;t understood yet !!!\nEquation (17) allows us to sequentially compute the approximation \\(u_i^m\\) at any point \\((x_i, t_m)\\), where \\(u_i^1 = u(x_i, 0), i = 1\\cdots N\\) were given by the initial and boundary conditions. In matrix notation, the series of equation can be written as:\n$$ \\begin{equation} \\begin{aligned} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ r \u0026amp; 1 - 2r \u0026amp; r \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp;\\vdots \u0026amp;\\ddots \u0026amp;\\vdots \u0026amp;\\vdots \u0026amp;\\vdots \\\\ 0 \u0026amp;0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; r \u0026amp; 1 - 2r \u0026amp; r \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\underbrace{\\begin{bmatrix} u_1^m \\\\ u_2^m \\\\ \\vdots \\\\ u_{N-1}^m \\\\ u_N^m \\\\ \\end{bmatrix}}_{u^m} = \\underbrace{\\begin{bmatrix} u_1^{m+1} \\\\ u_2^{m+1} \\\\ \\vdots \\\\ u_{N-1}^{m+1} \\\\ u_N^{m+1} \\\\ \\end{bmatrix}}_{u^{m+1}} \\end{aligned} \\end{equation} $$\nNote that \\(u_1^m\\), \\(u_N^m\\) are always equal to its value in the next time step. This is due to the boundary condition, the temperature at the boundary is always \\(0\\).\n(code) Implementation of FTCS scheme solve_fdm() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np def solve_fdm(N: int, M: int, T: float): \u0026#34;\u0026#34;\u0026#34; solving 1D heat equation: PDE: u_t = u_xx (\\alpha^2 = 1) BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = x - x**2 args: - N, M : number of collocation points in spacial and temporal dimension - T : solving from t = 0 to T \u0026#34;\u0026#34;\u0026#34; # constructing the grid dx = 1 / (N - 1) # 0 \u0026lt;= x \u0026lt;= 1 dt = T / (M - 1) # 0 \u0026lt; t \u0026lt;= T r = dt/dx**2 # (alpha = 1) # Condition for numerical stability assert r \u0026lt; .5, ValueError(f\u0026#34;Choose smaller r, r={r:.4f}\u0026#34;) x_grid = np.linspace(0, 1, N) # approximate the result U = np.zeros((N, M)) # already satisfied the BCs # IC impose initial condition ic = lambda x: np.sin(2 * np.pi * x) U[:, 0] = np.vectorize(ic)(x_grid) # kernel to approximate 2nd derivative of u wrt x ker = np.array([1., -2., 1.], dtype=np.float64) for i in range(1, M): ut = np.convolve(U[:, i - 1], ker, mode=\u0026#34;same\u0026#34;) U[:,i] = U[:, i-1] + r * ut return U 1 2 # solving the PDE with 100x4000 grid from t=0 to t=0.2 U = solve_fdm(100, 4000, .2) Backward Time, Centered Space (BTCS) Using First Order Backward Difference (equation 11) to approximate parital derivative of \\(u\\) at a grid point \\((x_i, t_m)\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{\\partial u}{\\partial t} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i, t_m) - u(x_i, t_m - \\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t)\\\\ \u0026amp; \\approx \\frac{u_i^{m}-u_i^{m-1}}{\\Delta t} \\end{aligned} \\end{equation} $$\nUsing Second Order Central Difference (equation 14) to approximate the second order partial derivative of \\(u\\) with respect to \\(x\\) at the grid point: $$ \\begin{equation} \\begin{aligned} \\frac{\\partial^2 u}{\\partial x^2} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i + \\Delta x, t_m) - 2 u(x_i, t_m) + u(x_i - \\Delta x, t_m)}{\\Delta x^2} + \\mathcal{O}(\\Delta x^2)\\\\ \u0026amp; \\approx \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2} \\end{aligned} \\end{equation} $$\nReplacing equation (19), and (20) into LHS and RHS of the PDE in (1) respectively we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{u_i^{m}-u_i^{m-1}}{\\Delta t} = \\alpha^2 \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2}\\\\ \\implies \u0026amp; u_i^{m-1} = u_i^m - \\frac{\\alpha^2 \\Delta t}{\\Delta x^2} (u_{i+1}^m - 2 u_i^m + u_{i-1}^m) \\\\ \u0026amp; = \\blue{u_i^m(1 + 2r) - r(u_{i+1}^m + u_{i-1}^m)} \\end{aligned} \\end{equation} $$\nWhere \\(r = \\frac{\\alpha^2\\Delta t}{\\Delta x^2}\\). Rewriting equation(21) in matrix notation:\n$$ \\begin{equation} \\begin{aligned} \\underbrace{\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ -r \u0026amp; 1 + 2r \u0026amp; -r \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp;\\vdots \u0026amp;\\ddots \u0026amp;\\vdots \u0026amp;\\vdots \u0026amp;\\vdots \\\\ 0 \u0026amp;0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; -r \u0026amp; 1 + 2r \u0026amp; -r \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}}_A \\underbrace{\\begin{bmatrix} u_1^m \\\\ u_2^m \\\\ \\vdots \\\\ u_{N-1}^m \\\\ u_N^m \\\\ \\end{bmatrix}}_{\\mathbf{u}^m} = \\underbrace{\\begin{bmatrix} u_1^{m-1} \\\\ u_2^{m-1} \\\\ \\vdots \\\\ u_{N-1}^{m-1} \\\\ u_N^{m-1} \\\\ \\end{bmatrix}}_{\\mathbf{u}^{m-1}} \\end{aligned} \\end{equation} $$\nSo that we can sequentially compute the next state by solving the system of linear equations in (22):\n$$ \\blue{ \\mathbf{u}^{m} = A^{-1} \\mathbf{u}^{m-1}; \\quad m = 2,\\cdots M } $$\nWhere \\(\\mathbf{u}_i^1\\) are given by the initial and boundary conditions. Unlike FTCS, BTCS are unconditionally stable with respect to the choice of \\(r\\). Therefore we can choose much fewer steps along temporal dimension.\n(code) Implementation of BTCS scheme solve_bdm() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np from scipy.sparse import diags def solve_bdm(N: int, M: int, T: float): \u0026#34;\u0026#34;\u0026#34; solving 1D heat equation using BTCS scheme PDE: u_t = u_xx (\\alpha^2 = 1) BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = x - x**2 args: - N, M : number of collocation points in spacial and temporal dimension - T : solving from t = 0 to T \u0026#34;\u0026#34;\u0026#34; # constructing the grid dx = 1 / (N - 1) # 0 \u0026lt;= x \u0026lt;= 1 dt = T / (M - 1) # 0 \u0026lt; t \u0026lt;= T r = dt/dx**2 # (alpha = 1) # construct A: A = diags([-r, 1 + 2 * r, -r], [-1, 0, 1], shape=(N, N)).toarray() A[0, :] = 0 A[-1, :] = 0 A[0, 0] = 1 A[-1,-1] = 1 A_inv = np.linalg.inv(A) # approximate the result U = np.zeros((N, M)) # already satisfied the BCs # IC impose initial condition x_grid = np.linspace(0, 1, N) ic = lambda x: np.sin(2 * np.pi * x) U[:, 0] = np.vectorize(ic)(x_grid) for m in range(1, M): U[:, m] = A_inv @ U[:, m-1] return U 1 2 # solving the PDE with 100x100 grid from t=0 to t=0.2 U = solve_bdm(100, 100, .2) We can see that results of FTCS and BTCS agree with each other, however, BTCS only using a \\(100 \\times 100\\) grid while FTCS using \\(100 \\times 4000\\) grid.\nThere is one more scheme for finite different methods which is Crank-Nicolson methods, which use central diffence method to estimate first order derivative !!\nPhysics Informed Neural Network Let\u0026rsquo;s rewrite the heat equation in a more general form:\n$$ \\begin{equation} \\begin{aligned} PDE: \u0026amp; \u0026amp; u_t = \\alpha^2 u_{xx} \u0026amp; \u0026amp; 0 \u0026lt; x\u0026lt; 1 \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ BCs: \u0026amp; \u0026amp; \\begin{cases} u(0, t) = f_0(t)\\\\ u(1, t) = f_1(t) \\end{cases} \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ IC: \u0026amp; \u0026amp; u(x, 0) = \\phi(t) \u0026amp; \u0026amp; 0 \\leq x \\leq 1 \\end{aligned} \\end{equation} $$\nIn our case, \\(f_0(t) = f_1(t) = 0\\) and \\(\\phi(x) = \\sin 2\\pi x\\). PINN approximate the function \\(u(x, t)\\) by a neural network \\(U_\\theta(x, t)\\), and then learn the networks parameters \\(\\theta\\) by minimize the loss function:\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}(\\theta) \u0026amp; = \\frac{1}{N}\\sum_{i=1}^N{[U_\\theta(x_i, t_i) - u_i]^2} \u0026amp; \\text{(Supervised loss)}\\\\ \u0026amp; + \\frac{\\lambda_j}{M_j} \\sum_{j=1}^{M_j}{\\bigg[ \\frac{\\partial U_\\theta}{\\partial t}- \\alpha^2 \\frac{\\partial^2 U_\\theta}{\\partial x^2} \\bigg](x_j, t_j)} \u0026amp; \\text{(PDE residual)}\\\\ \u0026amp; + \\frac{\\lambda_k}{M_k} \\sum_{k=1}^{M_k}{[(U_\\theta(0, t_k) - f_0(t_k))^2 + (U_\\theta(1, t_k) - f_1(t_k))^2]} \u0026amp; \\text{(Boundary conditions)} \\\\ \u0026amp; + \\frac{\\lambda_h}{M_h} \\sum_{h=1}^{M_h}{[U_\\theta(x_h, 0) - \\phi(x_h)]^2} \u0026amp; \\text{(Initial condition)} \\end{aligned} \\end{equation} $$\nThe first term is the supervised loss, coinciding with statistical machine learning. Where \\({(x_i, t_i, u_i)}_{i=1\\cdots N}\\) is the set of collocation points \\((x_i, t_i)\\), and value of \\(u_i = u(x_i, t_i)\\).\nThe second term is the PDE residual, where:\n\\(\\frac{\\partial U_\\theta}{\\partial t}\\) is the partial derivative of the network \\(U_\\theta\\) with respect to the time input \\(t\\) Similarly, \\(\\frac{\\partial^2 U_\\theta}{\\partial x^2}\\) is the second derivative of the network with repsect to location \\(x\\). Second and third terms are the initial and boundary conditions, given by equation (23).\nWe don\u0026rsquo;t necessarily have access to the first loss term. In the implementation bellow, I ignored the first loss term. For the remaining three loss terms:\nPDE residual: \\(x_j \\sim \\text{Uniform}(0, 1); t_j \\sim\\text{Uniform}(0, 0.2)\\) Boundary condition: \\(t_k \\sim \\text{Uniform}(0, 0.2)\\) Initial condition: \\(x_h \\sim \\text{Uniform}(0, 1)\\) Note: The code need some refactoring but it still works.\n(code) JAX Implementation of PINN: train(), and compute_grid() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 import numpy as np import click import equinox as eqx import jax import jax.numpy as jnp import optax import yaml from matplotlib import pyplot as plt from pydantic import BaseModel from pydantic import PositiveInt from tqdm import trange from src.viz import plot_heatmap from src.viz import animate # Plot config plt.style.use(\u0026#39;ggplot\u0026#39;) class PINNConfig(BaseModel): key: int layers: list[PositiveInt] batch_ic_size: PositiveInt batch_bc_size: PositiveInt batch_interior_size: PositiveInt fn_path: str fn_path_gif: str class U(eqx.Module): \u0026#34;\u0026#34;\u0026#34; Simple MLP taking (x, t) as input, return mlp(x, t) \u0026#34;\u0026#34;\u0026#34; layers: list # def __init__(self, layers: list[int], key): self.layers = [] for _in, _out in zip(layers[:-1], layers[1:]): key, subkey = jax.random.split(key, 2) self.layers.append(eqx.nn.Linear(_in, _out, key=subkey)) def __call__(self, x, t): \u0026#34;\u0026#34;\u0026#34; assuming x in R^{n x (d - 1)}, t in R \u0026#34;\u0026#34;\u0026#34; out = jnp.concatenate([x, t], axis=-1) for layer in self.layers[:-1]: out = layer(out) out = jax.nn.tanh(out) out = self.layers[-1](out) return jax.nn.tanh(out) def interior_loss(u, x, t): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; # First and second derivative of u wrt x u_x = jax.grad(lambda x, t: jnp.squeeze(u(x, t)), argnums=0) u_xx = jax.grad(lambda x, t: jnp.squeeze(u_x(x, t)), argnums=0) u_t = jax.grad(lambda x, t: jnp.squeeze(u(x, t)), argnums=1) pde_resid = jax.vmap(u_t)(x, t) - jax.vmap(u_xx)(x, t) return jnp.mean(pde_resid**2) def boundary_loss(u, x, t, f_bc: callable): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; # compute boundary value at each collocation point y = jax.vmap(f_bc)(x, t) y_hat = jax.vmap(u)(x, t) return jnp.mean((y - y_hat) ** 2) def initial_condition_loss(u, x, t, f_ic): y = jax.vmap(f_ic)(x, t) y_hat = jax.vmap(u)(x, t) return jnp.mean((y - y_hat) ** 2) def generate_interior_batch(key, n): \u0026#34;\u0026#34;\u0026#34; interior collocation points \u0026#34;\u0026#34;\u0026#34; # sample, discretizing interior point key, subkey = jax.random.split(key, 2) X = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=1-1e-5) key, subkey = jax.random.split(key, 2) T = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=.2) return X, T def generate_ic_batch(key, n): \u0026#34;\u0026#34;\u0026#34; initial collocation points {(x_i, 0)} \u0026#34;\u0026#34;\u0026#34; # sample, discretizing interior point key, subkey = jax.random.split(key, 2) X = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=1-1e-5) key, subkey = jax.random.split(key, 2) T = jnp.zeros(shape=(n, 1)) return X, T def generate_bc_batch(key, n): \u0026#34;\u0026#34;\u0026#34; initial collocation points {(0/1, t_m)} \u0026#34;\u0026#34;\u0026#34; # sample, discretizing interior point key, subkey = jax.random.split(key, 2) X = jax.random.randint(subkey, shape=(n, 1), minval=0., maxval=2.) key, subkey = jax.random.split(key, 2) T = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=.2) return X, T def loss_fn(u, x_i, t_i, x_ic, t_ic, x_bc, t_bc, f_ic, f_bc): \u0026#34;\u0026#34;\u0026#34; u: model x_i, t_i: interior collocation point x_ic, t_ic: initial points x_bc, t_bc: boundar points f_ic: initial condition f_bc: boundary condition \u0026#34;\u0026#34;\u0026#34; return interior_loss(u, x_i, t_i) +\\ initial_condition_loss(u, x_ic, t_ic, f_ic) +\\ boundary_loss(u, x_bc, t_bc, f_bc) def train(config: PINNConfig): key = jax.random.PRNGKey(config.key) key, subkey = jax.random.split(key, 2) # define the model u = U(config.layers, subkey) # define initial condition def f_ic(x, t): return jnp.sin(2 * jnp.pi * x) # define boundary condition def f_bc(x, t): return 0. # compute loss \u0026amp; loss gradient grad_loss_fn = jax.value_and_grad(loss_fn) @jax.jit def train_step(model, key, optim_state): # Generate data point ic_key, bc_key, i_key = jax.random.split(key, 3) x_ic, t_ic = generate_ic_batch(ic_key, config.batch_ic_size) x_bc, t_bc = generate_bc_batch(bc_key, config.batch_bc_size) x_i, t_i = generate_interior_batch(i_key, config.batch_interior_size) loss_val, grads = grad_loss_fn( model, x_i, t_i, x_ic, t_ic, x_bc, t_bc, f_ic, f_bc) updates, optim_state = optim.update(grads, optim_state) new_model = eqx.apply_updates(model, updates) return loss_val, new_model, key, optim_state # optimizer optim = optax.adam(1e-3) optim_state = optim.init(u) losses = [] pbar = trange(10000) for i in pbar: loss, u, key, optim_state = train_step(u, key, optim_state) pbar.set_description(f\u0026#34;Loss = {loss:.4f}\u0026#34;) losses.append(loss) losses = jnp.array(losses) return u, losses def compute_grid(model: eqx.Module, config: PINNConfig): \u0026#34;\u0026#34;\u0026#34; compute model output on 100 x 100 grid over following domain x in (0, 1) t in (0, 0.2) \u0026#34;\u0026#34;\u0026#34; def _compute(x, t): # transform scalar into a 1d vector x -\u0026gt; [x] x, t = jnp.expand_dims(x, 0),\\ jnp.expand_dims(t, 0) u = model(x, t) u = jnp.squeeze(u) return u x = jnp.linspace(0, 1, 100) t = jnp.linspace(0, 0.2, 100) xx, tt = np.meshgrid(x, t, sparse=True) U = np.vectorize(_compute)(xx, tt).T return U 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026#34;\u0026#34;\u0026#34; # Config params: key: 0 layers: [2, 128, 64, 32, 1] batch_ic_size: 16 batch_bc_size: 16 batch_interior_size: 128 \u0026#34;\u0026#34;\u0026#34; @click.command() @click.option(\u0026#34;--config\u0026#34;, \u0026#34;-C\u0026#34;, type=str, required=True, help=\u0026#34;path/to/config\u0026#34;) def main(config: str): \u0026#34;\u0026#34;\u0026#34; Solving PDE using PINN PDE: u_t = alpha^2 u_{xx} 0 \u0026lt; x \u0026lt; 1; 0 \u0026lt; t \u0026lt; \\infty BCs: u(0, t) = u(1, t) = 0 0 \u0026lt; t \u0026lt; \\infty ICs: u(x, 0) = sin(2pi x) 0 \u0026lt; x \u0026lt; 1 Params: alpha^2 = 1 \u0026#34;\u0026#34;\u0026#34; with open(config, \u0026#34;r\u0026#34;) as f: config = yaml.safe_load(f) config = PINNConfig(**config[\u0026#34;params\u0026#34;]) model, losses = train(config) U = compute_grid(model, config) plot_heatmap(U, config.fn_path) animate(U, config.fn_path_gif) return ","permalink":"http://localhost:1313/posts/heat/","summary":"Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:\n(Book) Partial Differential Equations for Scientists and Engineers - Standley J. Farlow for deriving closed-form solution. (Article) Finite-Difference Approximations to the Heat Equation (Course) ETH Zurich | Deep Learning for Scientific Computing 2023 for Theory and Implementation of Physics-Informed Neural Network. Introduction Physics-Informed Machine Learning (PIML) is an exciting subfield of Machine Learning that aims to incorporate physical laws and/or constraints into statistical machine learning.","title":"Learning to solve heat equation"},{"content":"Outline\nODE with initial value problem Reduction of orders Euler method ","permalink":"http://localhost:1313/posts/pde/","summary":"Outline\nODE with initial value problem Reduction of orders Euler method ","title":"Numerical methods for PDE"},{"content":"Update 2024 May Accepted into a graduate program. Apr Currently looking for PhD Opportunity in the field of AI/ML. My research interest lies in Reinforcement Learning and Physics-Informed Machine Learning. 2022 Dec I am 29. My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence. Resumes / Portfolio Academic Resume Research portfolio Professional Resume Researchs / Publications Accepted (BME 2020) Tu, Do Thanh, Thuong Nguyen, Anh Tho Le, Sinh Nguyen, Huong Ha. \u0026ldquo;Automated EOG removal from EEG signal using Independent Component Analysis and Machine Learning Algorithms\u0026rdquo; at The 8th International Conference in Vietnam on the Development of Biomedical Engineering. (link)\n(ICHST 2023) Tu, Do Thanh, Luan Van Tran, Tho Anh Le, Thao Mai Thi Le, LanAnh Hoang Duong, Thuong Hoai Nguyen, Anh Minh Hoang An, Duy The Phan, Khiet Thu Thi Dang, Quyen Hoang Quoc Vo, Nam Phuong Nguyen, Huong Thanh Thi Ha. \u0026ldquo;Stress prediction using machinelearning technique on physiological signal\u0026rdquo; (link)\n(IJCNN 2024) Tu T. Do, Mai Anh Vu, Hoang Thien Ly, Thu Nguyen, Steven A. Hicks, Michael A. Riegler, Pl Halvorsen Halvorsen and Binh T. Nguyen. \u0026ldquo;Blockwise Principal Component Analysis for monotone missing data imputation and dimensionalityreduction\u0026rdquo;\nSubmitted Mai Anh Vu , Thu Nguyen , Tu T. Do , Nhan Phan, Nitesh V. Chawla, Pl Halvorsen, Michael A. Riegler and Binh T. Nguyen. \u0026ldquo;Conditional expectation with regularization for missing data imputation\u0026rdquo;\nTu T. Do, Mai Anh Vu, Hoang Thien Ly, Thu Nguyen, Steven A. Hicks, Michael A. Riegler, Pl Halvorsen Halvorsen and Binh T. Nguyen. \u0026ldquo;Estimating lowerdimensional space representation in Principal Component Analysis under missing data condition\u0026rdquo;\nContact Email tu.dothanh1906@gmail.com Phone (+84) 343 610 772 Github young1906 Facebook tu.dothanh ","permalink":"http://localhost:1313/about/","summary":"Update 2024 May Accepted into a graduate program. Apr Currently looking for PhD Opportunity in the field of AI/ML. My research interest lies in Reinforcement Learning and Physics-Informed Machine Learning. 2022 Dec I am 29. My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence. Resumes / Portfolio Academic Resume Research portfolio Professional Resume Researchs / Publications Accepted (BME 2020) Tu, Do Thanh, Thuong Nguyen, Anh Tho Le, Sinh Nguyen, Huong Ha.","title":"About"},{"content":"My goal is to familiarize myself with physics-informed neural network(PINN), consist of \u0026hellip; Having no formal training on solving PDE, the first section will outline basic concepts in solving partial differential equations (PDEs), an example of a PDE, and its closed-form solution.\nOutline:\nPDE Closed-formed solution Generating data PINN PDE and the Legendre Differential Equation This section is written with instruction from this introduction by JD Moore.\nDefinition (Power series)\n$$ \\sum_{n=0}^\\infty{a_n(x - x_0)^n} $$\nTheorem For any power series centered at \\(x_0\\), there exists a non-negative real number \\(R\\) or \\(\\infty\\), such that\nThe power series converges when \\(|x - x_0| \u0026lt; R\\) and diverges when \\(|x - x_0| \u0026gt; R\\) We refer \\(R\\) as radius of convergence.\nNote\nDefinition (Ratio test) The radius of convergence of a power series\n$$ \\sum_{n=0}^\\infty{a_n(x - x_0)^n} $$\nis given by the formula\n$$ R = \\lim_{n\\rightarrow \\infty} \\frac{|a_n|}{|a_{n+1}|} $$\nDefinition (Comparison test) Suppose that the power series\n$$ \\begin{aligned} \\sum_{n=0}^\\infty{a_n(x - x_0)^n},\\quad \\sum_{n=0}^\\infty{b_n(x - x_0)^n} \\end{aligned} $$\nDefinition (Singular point)\nDefinition (Regular singular point)\nDefinition (Ordinary singular point)\nLegendre differential equation The Legendre differential equation (2) is the second-order ordinary differential equation , which has the form:\n$$ \\begin{equation} (1 - x^2) \\frac{d^2y}{dx^2} - 2x\\frac{dy}{dx} + p(p+1) y = 0 \\end{equation} $$\nWhere \\(p\\) is a parameter.\nSolving Legendre differential equation Assume \\(y\\) has the form of a power series centered at \\(0\\)\n$$ \\begin{aligned} y \u0026amp;= \\sum_{n=0}^\\infty{a_n x^n}\\\\ \\frac{dy}{dx} \u0026amp;= \\sum_{n=1}^\\infty{n a_n x^{n-1}}\\\\ \\frac{d^2y}{dx^2} \u0026amp;= \\sum_{n=2}^\\infty{n(n-1)a_n x^{n-2}}\\\\ \\end{aligned} $$\nSubsitute into (eq. 1) we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; (1 - x^2)\\sum_{n=2}^\\infty{n(n-1)a_n x^{n-2}} - 2x \\sum_{n=1}^\\infty{n a_n x^{n-1}} + p(p+1)\\sum_{n=0}^\\infty{a_n x^n} \u0026amp;= 0 \\\\ \\implies \u0026amp; \\underbrace{ \\sum_{n=2}^\\infty{n(n-1)a_n x^{n-2}} }_{A} - \\underbrace{ \\sum_{n=2}^\\infty{n(n-1)a_n x^n} }_{B} - 2 \\underbrace{ \\sum_{n=1}^\\infty{n a_n x^n} }_{C} + p(p+1)\\sum_{n=0}^\\infty{a_n x^n} \u0026amp; = 0 \\end{aligned} \\end{equation} $$\nTerm \\(A\\): $$ \\begin{aligned} A \u0026amp;= \\sum_{n=2}^\\infty{n(n-1)a_n x^{n-2}} \\\\ \u0026amp;= \\sum_{m=0}^\\infty (m+1)(m+2)a_{m+2} x^m \u0026amp; \\text{(replacing } n = m + 2 \\text{)}\\\\ \u0026amp;= \\sum_{n=0}^\\infty (n+1)(n+2)a_{n+2} x^n \u0026amp; \\text{(because m is a dummy index)} \\end{aligned} $$\nTerm \\(B\\) $$ \\begin{aligned} B \u0026amp; = \\sum_{n=2}^\\infty{n(n-1)a_n x^n} \\\\ \u0026amp;= 0 (0 - 1) a_0 x^0 + 1 ( 1- 1) a_1 x^1 + \\sum_{n=2}^\\infty{n(n-1)a_n x^n}\\\\ \u0026amp;= \\sum_{n=0}^\\infty{n(n-1)x^n} \\end{aligned} $$\nSame trick with term \\(C\\) $$ C = \\sum_{n=0}^\\infty{n a_n x^n} $$\nSubstitute \\(A, B, C\\) into eq. 2, we have\n$$ \\sum_{n=0}^\\infty{ \\big[ \\underbrace{ (n+1)(n+2)a_{n+2} - n(n-1)a_n - 2na_n + p(p+1)a_n }_{=0} \\big] x^n } = 0 $$\nThe equality above is satisifed iff each coefficient of the polinomial is \\(0\\). So we can obtains the recursion form of coefficients\n$$ a_{n+2} = \\frac{n(n+1) - p(p+1)}{(n+1)(n+2)}a_n = \\frac{(n-p)(p+n+1)}{(n+1)(n+2)}a_n $$\nWhere \\(a_0, a_1\\) are given by initial conditions \\(y(0), \\frac{dy}{dx}(0)\\).\nFor even \\(n\\):\n\\(n = 0, a_2 = -\\frac{p(p+1)}{1\\times 2}a_0 \\) \\(n = 2, a_4 = -\\frac{(p-2)(p+3)}{3 \\times 4}a_2 = \\frac{p(p+1)(p-2)(p+3)}{4!}a_0 \\) \\(n = 4, a_6 = -\\frac{(p-4)(p+5)}{5 \\times 6}a_4 = -\\frac{p(p+1)(p-2)(p+3)(p-4)(p+5)}{6!} a_0 \\) \\(\\cdots\\) So that\n$$ a_{2k} = (-1)^{k+1} \\frac{p(p+1)(p-2)\\cdots (p-2k)(p+2k+1)}{(2k+2)!} $$\nSimilary for odd \\(n\\)\n\\(n = 1, a_3 = -\\frac{(p - 1)(p+2)}{2\\times 3}a_1 \\) \\(n = 3, a_5 = -\\frac{(p - 3)(p+4)}{4\\times 5}a_3 = \\frac{(p-1)(p+2)(p-3)(p+4)}{5!}a_1\\) \\(n = 5, a_7 = -\\frac{(p - 5)(p+6)}{6\\times 7}a_5 = -\\frac{(p-1)(p+2)(p-3)(p+4)(p-5)(p+6)}{7!}a_1 \\) \\(\\cdots\\) So that\n$$ a_{2k+1} = (-1)^{k+1} \\frac{(p-1)(p+2)\\cdots (p-2k+1)(p+2k+2)}{(2k+1)!} $$\nTODO:\nPerform ratio test to analyze radius of convergence. Analysis of singular points? Using PINN to solve PDE Construction / Components Dataset {X, y} Parameterize \\(y = f_\\theta(x)\\) where \\(f\\) is a neural network. Define loss function: \\(\\text{MSE} = \\text{MSE}_f + \\text{MSE}_e\\) Comparing PINN to ordinary Multi-Layer Perceptron (MLP) Implemetation Training Evaluation References (1) Introduction to Partial Differential Equation - John Douglas Moore (2) https://mathworld.wolfram.com TODO: Understand the ratio test ","permalink":"http://localhost:1313/posts/pinn/","summary":"My goal is to familiarize myself with physics-informed neural network(PINN), consist of \u0026hellip; Having no formal training on solving PDE, the first section will outline basic concepts in solving partial differential equations (PDEs), an example of a PDE, and its closed-form solution.\nOutline:\nPDE Closed-formed solution Generating data PINN PDE and the Legendre Differential Equation This section is written with instruction from this introduction by JD Moore.\nDefinition (Power series)","title":"Learning physics-informed Neural Networks (PINN)"},{"content":"Problem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$\nAs an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \\(p_A(H) = p \\text{ and } p_B(H) = q\\). For each trial, we select coin A with probability \\(p(A) = \\tau\\) and coin B with probability \\(p(B) = 1 -\\tau\\), toss the coin and record the observation. The set of observations \\(\\boldsymbol{X}\\) is the record of head or tail \\(\\{H, T, H, H, \\cdots\\}\\), the latent variable which is unobserved is which coint is selected for each trail \\(\\{A, B, B, A, \\cdots\\}\\), and the unknown parameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that best fit observations; EM is an instance of Maximum Likelihood Estimation (MLE).\nThe EM algorithm The algorithm The EM algorithm seeks for \\(\\boldsymbol{\\theta}\\) by first initiates a random parameter vector \\(\\boldsymbol{\\theta}^{(0)}\\) and then iteratively performs two steps, namely the expectation step (E step) and the maximization step (M step):\n(The E step) the expected loglikelihood of \\(\\boldsymbol{\\theta}\\), with respect to the current conditional distribution of \\(\\boldsymbol{Z}\\) given observations \\(\\boldsymbol{X}\\) and current estimation of \\(\\boldsymbol{\\theta}^{(t)}\\) $$ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}_{\\boldsymbol{Z} \\sim P(. | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)})} {[ \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) ]} $$\n(The M step) update parameter vector \\(\\boldsymbol{\\theta}\\) $$ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) $$\nProof of correctness Setup We need to proof that updating parameter vector \\(\\boldsymbol{\\theta}\\) by EM algorithm will monotonically increase the marginal likelihood of \\(P(X|\\theta)\\)\n$$ \\log P(X|\\theta^*) - \\log P(X|\\theta^{(t)}) \\geq Q(\\theta^* | \\theta^{(t)}) - Q(\\theta^{(t)} | \\theta^{(t)}) \\geq 0 $$ Where the second inequality come from \\(\\theta^* = \\arg\\max_\\theta Q(\\theta | \\theta^{(t)}) \\)\nProof $$ \\begin{aligned} \u0026amp; P(X, Z | \\theta) = P(Z | X, \\theta) P(X | \\theta) \u0026amp; \\text{\\tiny(Bayes theorem)} \\\\ \\iff \u0026amp; \\log P(X, Z | \\theta) = \\log P(Z |X, \\theta) + \\log P(X |\\theta) \u0026amp; \\\\ \\iff \u0026amp; \\log P(X | \\theta) = \\log P(X, Z | \\theta) - \\log P (Z | X,\\theta) \u0026amp; \\\\ \\implies \u0026amp; \\log P(X | \\theta) = \\underbrace{ \\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(X, Z | \\theta)]} }_{Q(\\theta | \\theta^{(t)})} + \\underbrace{ - \\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(Z|X,\\theta)]} }_{H(\\theta | \\theta^{(t)})} \u0026amp; \\text{\\tiny(taking expectation for both side)} \\end{aligned} $$\nSo consider \\(\\log P(X|\\theta) - \\log P(X|\\theta^{(t)})\\) is the change in loglikelihood of observed data when we update the parameter vector \\(\\theta\\)\n$$ \\begin{aligned} \\log P(X|\\theta) - \\log P(X|\\theta^{(t)}) \u0026amp; = Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) \\\\ \u0026amp; + \\underbrace{H(\\theta|\\theta^{(t)}) - H(\\theta^{(t)}|\\theta^{(t)})}_{A} \\\\ \\end{aligned} $$\nQuantity \\(A\\)\n$$ \\begin{aligned} A \u0026amp; = - \\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(Z|X,\\theta)]} - \\big( -\\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(Z|X,\\theta^{(t)})]} \\big) \\\\ \u0026amp; = \\mathbb{E}_{Z|X,\\theta^{(t)}}{\\bigg[ \\log P(Z|X,\\theta^{(t)}) - \\log P(Z|X,\\theta) \\bigg]} \\\\ \u0026amp; = \\int_{Z} P(Z|X,\\theta^{(t)}) \\log{\\frac{P(Z|X,\\theta^{(t)})}{P(Z|X,\\theta)} dZ}\\\\ \u0026amp; \\geq 0 \u0026amp;\\text{\\tiny(Gibb\u0026rsquo;s inequality)} \\end{aligned} $$\nSo that\n$$ \\begin{aligned} \\log P(X|\\theta) - \\log P(X|\\theta^{(t)}) \u0026amp; = Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) + A \\\\ \u0026amp; \\geq Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) \u0026amp; \\square \\end{aligned} $$\nExamples EM for the coin example Setup\nParameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\), and its estimation at step (t) is \\(\\boldsymbol{\\theta}^{(t)} = [p_t, q_t, \\tau_t]\\)\nThe \\(i^{th}\\) observation \\(x^{(i)}\\) is either head (H) or tail (T).\nThe coin selected for the \\(i^{th}\\) trail \\(z^{(i)}\\) is either A or B:\n\\(p(z^{(i)} = A) = \\tau\\) \\(p(z^{(i)} = B) = 1 -\\tau\\). For both cases, $$ \\begin{equation} p(z^{(i)}) = \\tau^{\\mathbb{I}(z^{(i)}=A)}(1-\\tau)^{\\mathbb{I}(z^{(i)}=B)} \\end{equation} $$\nWhen selected the coin A,\nProbability that we get a head (H): \\(p(x^{(i)}=H | z^{(i)} = A) = p\\) Probability that we get a head (T): \\(p(x^{(i)}=T | z^{(i)} = A) = 1 - p\\) For both cases, $$ \\begin{equation} p(x^{(i)} | z^{(i)}=A) = p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\nSimilarly, when B is selected $$ \\begin{equation} p(x^{(i)} | z^{(i)}=B) = q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\nWhere \\(\\mathbb{I}(\\cdot)\\) is an indicator function on a predicate $$ \\mathbb{I}(p) = \\begin{cases} 1 \\quad \\text{if } p \\text{ is True}\\\\ 0 \\quad \\text{otherwise} \\end{cases} $$\nOnce again, we generalize for both cases of \\(z^{(i)}\\)\n$$ \\begin{equation} \\begin{aligned} p(x^{(i)} | z^{(i)}) = [p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=A)}\\\\ \\times [q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=B)} \\end{aligned} \\end{equation} $$\nThe equation looks rather ugly, we can simplify this by encoding head as 1 and tail as 0; coin A as 1 and coin B as 0. The equation above can be written as\n$$ \\begin{equation} p(x^{(i)} | z^{(i)}) = [p^{x^{(i)}}(1-p)^{1 - x^{(i)}}]^{z^{(i)}} [q^{x^{(i)}}(1-q)^{1 - x^{(i)}}]^{1-z^{(i)}} \\end{equation} $$\nSimilarly for \\(p(z^{(i)})\\) $$ \\begin{equation} p(z^{(i)}) = \\tau^{z^{(i)}}(1-\\tau)^{1-z^{(i)}} \\end{equation} $$\nApplying EM algorithm\nThe (E step):\nConstruct the joint likelihood of a single pair of observation and latent variable \\(p(x^{(i)}, z^{(i)})\\ | \\boldsymbol{\\theta})\\). For the conciseness, we drop the \\((i)\\) superscript from the equation. $$ \\begin{equation} \\begin{aligned} p(x, z | \\boldsymbol{\\theta}) = \u0026amp; p(x | z, \\boldsymbol{\\theta})p(z | \\boldsymbol{\\theta})\\\\ = \u0026amp; [p^{x}(1-p)^{1 - x}]^{z} [q^{x}(1-q)^{1 - x}]^{1-z} \\tau^{z}(1-\\tau)^{1-z} \u0026amp; \\text{\\tiny(from eq. 5 and 6)} \\end{aligned} \\end{equation} $$\nLikelihood over entire observations \\(\\boldsymbol{X}\\) and latent \\(\\boldsymbol{Z}\\):\n$$\\boldsymbol{X}\\odot\\boldsymbol{Z} := \\{(x^{(i)}, z^{(i)})\\}_{i=1\\cdots N}$$\nA side note is that I am not entirely sure that \\(\\odot\\) operator is appropriate in this situation.\n$$ \\begin{equation} \\begin{aligned} P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) =\u0026amp; \\prod_{(x, z) \\in \\boldsymbol{X}\\odot\\boldsymbol{Z}} { p(x, z | \\boldsymbol{\\theta}) } \\end{aligned} \\end{equation} $$\nLog likelihood of the joint probability\n$$ \\begin{equation} \\begin{aligned} \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) \u0026amp; = \\sum_{(x, z)} \\log p(x, z | \\boldsymbol{\\theta}) \\end{aligned} \\end{equation} $$\nTaking a log always seem to make thing to be better.\nFinally, we need to take the expectation of the log likelihood w.r.t conditional probability of \\(\\boldsymbol{Z}|\\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}\\)\nPosterior for a single latent \\(z\\)\n$$ \\begin{equation} \\begin{aligned} p(z | x, \\boldsymbol{\\theta}^{(t)}) \u0026amp; = \\frac{p(x, z | \\boldsymbol{\\theta}^{(t)})} {p(x | \\boldsymbol{\\theta}^{(t)})} \u0026amp; \\text{\\tiny(Bayes Theorem)}\\\\ \u0026amp; = \\frac{p(x, z | \\boldsymbol{\\theta}^{(t)})} { p(x, z = 0| \\boldsymbol{\\theta}^{(t)}) + p(x, z = 1| \\boldsymbol{\\theta}^{(t)}) } \u0026amp; \\text{\\tiny(Marginal likelihood over z in denominator)}\\\\ \u0026amp; = \\frac{ [p_t^{x}(1-p_t)^{1 - x}]^{z} [q_t^{x}(1-q_t)^{1 - x}]^{1-z} \\tau_t^{z}(1-\\tau_t)^{1-z} }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \u0026amp; \\text{\\tiny(from eq. 7)} \\end{aligned} \\end{equation} $$\nTaking the expectation\n$$ \\begin{equation} \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) \u0026amp;= \\mathbb{E}_{\\boldsymbol{Z} | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}}{\\bigg[ \\sum_{(x, z)}{\\log p(x, z | \\boldsymbol{\\theta})} \\bigg]} \\\\ \u0026amp;= \\sum_{(x, z)} { \\mathbb{E}_{\\boldsymbol{Z} | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \u0026amp;= \\sum_{(x, z)} { \\mathbb{E}_{z | x, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \\end{aligned} \\end{equation} $$\nIt is always bothering for me that in literature, the posterior, of which to be taken expectation over, for the entire set latent variables \\(\\boldsymbol{Z} = \\{ z^{(1)}, \\cdots z^{(n)}\\}\\) can be replaced by the posterior for a single latent \\(z\\) in (eq. 11) without explanation. So in order to understand this, consider the equation.\n$$ \\begin{aligned} \\mathbb{E}_{\\boldsymbol{Z}}{\\bigg[\\sum_{z\\in \\boldsymbol{Z}}{f(z)}\\bigg]} \u0026amp;= \\int_{\\boldsymbol{Z}}{ \\bigg[\\sum_{z\\in\\boldsymbol{Z}} f(z)\\bigg] p(\\boldsymbol{Z}) d\\boldsymbol{Z} } \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}}{\\int_{\\boldsymbol{Z}}{f(z)}} p(\\boldsymbol{Z})d\\boldsymbol{Z} \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}}{ \\int_{\\boldsymbol{Z}\\text{/}z} \\underbrace{\\bigg[\\int_{z}f(z)p(z)dz\\bigg]}_{A=\\mathbb{E}_z[f(z)]} p(\\boldsymbol{Z}\\text{/}z)d(\\boldsymbol{Z}/z) } \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}} A \\int_{\\boldsymbol{Z}\\text{/}z} p(\\boldsymbol{Z}\\text{/}z)d(\\boldsymbol{Z}/z) \u0026amp; \\text{\\tiny(A is constant w.r.t variable being integrated over)} \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}} \\mathbb{E}_z[f(z)] \u0026amp; \\text{\\tiny(Integerating over a p.d.f evalulated to 1)} \\end{aligned} $$\nWhere \\(\\boldsymbol{Z} = \\{z^i\\}_{i=1\\cdots N}; z \\sim p(Z)\\); \\(\\boldsymbol{Z}/z\\) denotes set all variables within \\(\\boldsymbol{Z}\\) except \\(z\\).\nHaving clear that up, we are able to resume from (eq. 11) $$ \\begin{equation} \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) \u0026amp;= \\sum_{(x, z)} { \\mathbb{E}_{z | x, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \u0026amp;= \\sum_{(x, z)} {\\bigg[ p(z = 0 | x, \\boldsymbol{\\theta}^{(t)}) \\log p(x, z = 0 | \\boldsymbol{\\theta}) \\ + p(z = 1 | x, \\boldsymbol{\\theta}^{(t)}) \\log p(x, z = 1 | \\boldsymbol{\\theta}) \\bigg]} \\end{aligned} \\end{equation} $$\nFrom (eq. 7)\n$$ \\begin{aligned} p(x, z = 0 |\\boldsymbol{\\theta}) = q^x(1-q)^{1-x}(1-\\tau) \\end{aligned} $$ $$ \\begin{aligned} p(x, z = 1 |\\boldsymbol{\\theta}) = p^x(1-p)^{1-x}\\tau \\end{aligned} $$ From (eq. 10)\n$$ \\begin{aligned} p(z = 0 | x, \\boldsymbol{\\theta}^{(t)}) \u0026amp; = \\frac{ q_t^{x}(1-q_t)^{1-x}(1-\\tau_t) }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \\end{aligned} $$\n$$ \\begin{aligned} p(z = 1 | x, \\boldsymbol{\\theta}^{(t)}) \u0026amp; = \\frac{ p_t^{x}(1-p_t)^{1-x}\\tau_t }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \\end{aligned} $$\nThis probability is often referred as membership probability, denote the membership probability of the \\(i^{th}\\) observation \\(p(z^{(i)} = 0 | x^{(i)}, \\boldsymbol{\\theta}^{(t)}) = a_i\\) and \\(p(z^{(i)} = 1|x^{(i)},\\boldsymbol{\\theta}^{(t)}) = 1 - a_i\\). With this\nSubstitute these quantities into eq. 12 we have the expectation of the log-likelihood w.r.t conditional probability of \\(\\boldsymbol{Z}\\) given observations and current state of the parameters.\n$$ \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\sum_{i=1}^{N}{ a_i [x^{(i)}\\log q + (1-x^{(i)})\\log(1-q) + \\log(1-\\tau)] } \\\\ + (1 - a_i)[x^{(i)} \\log p + (1-x^{(i)})\\log (1-p) + log\\tau] \\end{aligned} $$\nThe M step $$ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}}{Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)})} $$\n\\(\\frac{\\partial Q}{\\partial p} = 0 \\)\n$$ \\begin{aligned} \u0026amp; \\frac{\\partial Q}{\\partial p} = 0 \\\\ \\iff \u0026amp; \\sum_{i=1}^N{(1-a_i)[\\frac{ x^{(i)}}{p}} - \\frac{1-x^{(i)}}{1-p}] = 0 \\\\ \\iff \u0026amp; \\frac{1}{p} \\underbrace{\\sum_{i=1}^N{(1-a_i)x^{(i)}}}_{A} = \\frac{1}{1-p} \\underbrace{\\sum_{i=1}^N{(1-a_i)(1-x^{(i)})}}_{B} \\\\ \\implies \u0026amp; p = A/(A+B) \\\\ \u0026amp; = \\color{red}{\\frac{\\sum_{i=1}^N{(1-a_i)x^{(i)}}}{\\sum_{i=1}^N{(1-a_i)x^{(i)}}+\\sum_{i=1}^N{(1-a_i)(1-x^{(i)})}}} \\end{aligned} $$\n\\(\\frac{\\partial Q}{\\partial q} = 0 \\), same with \\(p\\)\n$$ \\begin{aligned} \\color{red}{ q = \\frac{\\sum_{i=1}^N{a_i x^{(i)}}}{\\sum_{i=1}^N{a_i x^{(i)}}+\\sum_{i=1}^N{a_i(1-x^{(i)})}} } \\end{aligned} $$\n\\(\\frac{\\partial Q}{\\partial \\tau} = 0 \\) $$ \\begin{aligned} \u0026amp; \\frac{\\partial Q}{\\partial \\tau} = 0 \\\\ \\iff \u0026amp; \\sum_{1}^N{\\frac{-a_i}{1-\\tau} + \\frac{1-a_i}{\\tau}} = 0\\\\ \\iff \u0026amp; \\frac{1}{1-\\tau}\\sum_{1}^N{a_i} = \\frac{1}{\\tau}\\sum_{i=1}^N{(1-a_i)}\\\\ \\implies \u0026amp; \\color{red}{\\tau = \\frac{\\sum_{i=1}^N{1-a_i}}{N}} \\end{aligned} $$\nEM for Gaussian Mixture Model EM for GMM\u0026rsquo;s python implementation ","permalink":"http://localhost:1313/posts/em/","summary":"Problem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$\nAs an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \\(p_A(H) = p \\text{ and } p_B(H) = q\\).","title":"Expectation Maximization - EM"},{"content":"","permalink":"http://localhost:1313/posts/auto-grad/","summary":"","title":"Implementing Automatic Differentiation"},{"content":"This post is a note I take from while reading Blei et al 2018.\nGoal:\nMotivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= - \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\\\ \u0026amp;= -\\int_z{ q(z) \\log { \\frac{\\log p(z, x)}{q(z) p(x)} } }\\\\ \u0026amp;= -\\int_z{q(z)[\\log{\\frac{p(z,x)}{q(z)}} - \\log p(x)]dz} \\\\ \u0026amp;= -\\int_z{ q(z) \\log \\frac{p(z, x)}{q(z)}dz } + \\int_z{q(z)\\log p(x) dz} \\\\ \u0026amp; =: -\\texttt{ELBO}[q] + \\log p(x) \\\\ \\iff \\texttt{ELBO}[q] \u0026amp;= -KL(q||p) + \\log p(x) \\end{aligned} \\end{equation} $$\nBecause \\(\\log p(x)\\) is a constant, by maximizing \\(\\text{ELBO}[q]\\), we minimize \\(KL(q||p)\\) by proxy. Rewrite ELBO:\n$$ \\begin{equation} \\begin{aligned} \\texttt{ELBO}(q) \u0026amp;= \\int_z{q(z)\\log \\frac{p(z, x)}{q(z)}} \\\\ \u0026amp;= \\mathbb{E}_{z\\sim q}[\\log p(z, x)] - \\mathbb{E}_{z\\sim q}[\\log q(z)] \\end{aligned} \\end{equation} $$\nMean field Variational Family Mean-field variational family made a strong assumption of independence between it\u0026rsquo;s latent variable\n$$ q(\\mathbf{z}) = \\prod_{j} {q_j(z_j)} $$\nCoordinate ascent variational inference is a common method to solve mean-field variational inference problem. Holding other latent variable fixed, the \\(j^{th}\\) latent variable is given by:\n$$ q^*_{j}(z_j) = \\text{exp}{\\mathbb{E}_{-j}[\\log p(z_j | z_{-j}, \\mathbf{x})]} \\propto \\exp{\\mathbb{E}_{-j} [\\log p(z_j, z_{-j}, \\mathbf{x})]} $$\nProof $$ \\begin{equation} \\begin{aligned} q^*_j(z_j) \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad{\\texttt{ELBO}(q)} \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_q[\\log p(z_j, z_{-j}, x)] - \\mathbb{E}_q[\\log q(z_j, z_{-j})] \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log q_j(z_j) + \\log q_{-j}(z_{-j})]] \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] + const \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\end{aligned} \\end{equation} $$\nWe need to find function \\(q_j(z_j)\\) that maximize \\(\\text{ELBO}(q)\\)\nAssuming \\(q_j(z_j)= \\epsilon \\eta(z_j) + q^*_j(z_j)\\)\n$$ \\begin{aligned} K(\\epsilon) \u0026amp;= \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\\\ \u0026amp;= \\int_{z_j} q_j(z_j) A d_{z_j} - \\int_{z_j}q_j(z_j)\\log q_z(z_j) d_{z_j} \\\\ \u0026amp;= \\int_{z_j} [\\epsilon \\eta(z_j) + q^*_j(z_j)] A d_{z_j} - \\int_{z_j}[\\epsilon \\eta(z_j) + q^*_j(z_j)] \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)] d_{z_j} \\end{aligned} $$\nEvaluate the partial derivative of \\(K\\) wrt \\(\\epsilon\\) we have:\n$$ \\begin{aligned} \u0026amp; \\frac{\\partial}{\\partial \\epsilon}K \\bigg\\vert_{\\epsilon=0} = 0 \\\\ \\iff \u0026amp; \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j} { {\\eta(z_j) \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)]} + [\\epsilon \\eta(z_j) + q^*_j(z_j)] \\frac{\\eta(z_j)}{\\epsilon \\eta(z_j) + q^*_j(z_j)}d_{z_j} } = 0\\\\ \\iff \u0026amp; \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j}{[\\eta(z_j)\\log q^*_j(z_j) +\\eta(z_j)]d_{z_j}} = 0; \\quad \\forall \\eta(z_j) \\\\ \\iff \u0026amp; \\log q^*_j(z_j) = A-1 = \\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)] - 1 \\\\ \\iff \u0026amp; q^*_j(z_j) \\propto \\exp{\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]} \\end{aligned} $$\nComplete example of Bayesian Gaussian Mixture TDB\n","permalink":"http://localhost:1313/posts/variational_inference/","summary":"This post is a note I take from while reading Blei et al 2018.\nGoal:\nMotivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.","title":"Understanding Variational Inference"},{"content":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure 1 2 3 4 5 6 7 8 import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.log(x) vG = np.vectorize(g) x = np.random.uniform(0, 1, 10000) return vG(x).mean() ","permalink":"http://localhost:1313/posts/diff-under-integral-sign/","summary":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure 1 2 3 4 5 6 7 8 import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.","title":"Differentiation under integral sign"},{"content":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$\nThe second quantity \\(B\\):\n$$ \\begin{aligned} B =\u0026amp; \\int_x{p(x)\\big[ -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2 \\big]dx}\\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}\\int_x{ p(x)\\big[ (x - \\mu_1)^2 + 2(x-\\mu_1)(\\mu_1 - \\mu_2) + (\\mu_1 -\\mu_2)^2 \\big]dx} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 \\\\ \u0026amp; - \\frac{1}{2\\sigma_2^2}\\underbrace{\\int_x{p(x)(x-\\mu_1)^2}}_{\\text{var}(x)}\\\\ \u0026amp; - \\frac{2(\\mu_1 -\\mu_2)}{2\\sigma_2^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)dx}}_0 \\\\ \u0026amp; - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\end{aligned} $$\nFinally, we obtained the KL divergence for univariate case\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= A - B \\\\ \u0026amp;= (-\\frac{1}{2}\\log2\\pi - \\log\\sigma_1 - \\frac{1}{2}) - ( -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}) \\\\ \u0026amp;= \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2}{2\\sigma_2^2} + \\frac{(\\mu_1 -\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\end{aligned} $$\nMultivariate case tbd\nReference https://gregorygundersen.com/blog/ ","permalink":"http://localhost:1313/posts/closed-form-kl-gaussian/","summary":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$","title":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution"},{"content":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper. In the analogy above, the black box represents the simulator, and the configurations are the simulators parameters.\nThe applicability of SBI has great potential since we can almost reduce any process with defined input and output to a black-box machine 1.\nThis post documents my notes while studying Likelihood-free MCMC with Amortized Ratio Estimator (Hermans et al, 2020); a method developed to address SBI.\nLikelihood-free MCMC with Amortized Ratio Estimator Likelihood ratio is defined as the ratio between the likelihood of the observation between two different hypothesis:\n$$ r(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x}|\\theta_1)} $$\nThis quantity then can be used in various methods to draw sample from a distribution. In the paper, the author mention three sampling methods, namely Markov Chain Monte Carlo, Metropolis-Hasting, and HMC. In the following section, I am briefly summarizing those methods.\nBackground Markov Chain Monte Carlo (MCMC) In statistics, the MCMC method is a class of algorithms for sampling from a probability distribution. By constructing a Markov chain with the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the state chain 2.\nAdapting MCMC for SBI task We want to sample from \\(p(\\theta | \\mathbf{x})\\) using MCMC, we need this quantity\n$$ \\begin{equation} \\begin{aligned} \\frac{p(\\theta | \\mathbf{x} )}{p(\\theta_t| \\mathbf{x})} = \\frac{ p(\\theta)p(\\mathbf{x} | \\theta)/p(\\mathbf{x}) }{ p(\\theta_t)p(\\mathbf{x} | \\theta_t)/p(\\mathbf{x}) } = \\frac{p(\\theta)}{p(\\theta_t)}\\times \\frac{p(\\mathbf{x} | \\theta)}{p(\\mathbf{x} | \\theta_t)} = \\frac{p(\\theta)}{p(\\theta_t)} \\times r(\\mathbf{x} | \\theta, \\theta_t) \\end{aligned} \\end{equation} $$\nWe can compute the first term of the equation since we have access to prior \\(p(\\theta)\\). But we can not compute the second term because we do not have access to the likelihood function \\(p(\\mathbf{x} | \\theta)\\). However, we can reframe the problem in the supervised-learning paradigm, so we can use a parameterized discriminator \\(d_\\theta(\\mathbf{x})\\) to estimate the likelihood. The details are described in the Likelihood Ratio Estimator section.\nMetropolis-Hasting (MH) tbd\nHalmitonian Markov Chain(MH) tbd\nLikelihood Ratio Estimator The remaining question is how to estimate the likelihood ratio \\( r(\\mathbf{x} | \\theta_0, \\theta_1)\\). To estimate the ratio, the author employed the Likelihood Ratio Trick, training a discriminator \\(d_\\phi(\\mathbf{x})\\) to classify samples \\( x \\sim p(\\mathbf{x} | \\theta_0)\\) with class label \\(y = 1\\) from \\(\\mathbf{x} \\sim p(\\mathbf{x} | \\theta_1)\\) with class label \\(y = 0\\). The decision function obtained by the trained discrimininator:\n$$ d^*(\\mathbf{x}) = p(y = 1 | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x} | \\theta_0) + p(\\mathbf{x} | \\theta_1)} $$\nThen the estimation of likelihood ratio can be computed by:\n$$ \\hat{r}(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac {d^{*}(\\mathbf{x})} {1 - d^{*}(\\mathbf{x})} $$\nHowever, this method required the discriminator to be trained at every pair of \\((\\theta_0, \\theta_1)\\), which is impractical in the context. To overcome this issue, the paper proposed to train the discriminator to classify dependent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x}, \\mathbf{\\theta})\\) with label \\(y=1\\) from the independent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x})p(\\mathbf{\\theta})\\) with label \\(y=0\\).\n$$ \\begin{equation} \\begin{aligned} d^*(\\mathbf{x}, \\mathbf{\\theta}) \u0026amp;= \\frac {p(\\mathbf{x}, \\mathbf{\\theta})} { p(\\mathbf{x}, \\mathbf{\\theta}) + p(\\mathbf{x}) p(\\mathbf{\\theta}) } \\ \\end{aligned} \\end{equation} $$\nThe likelihood-to-evidence ratio is computed by\n$$ r(\\mathbf{x} | \\theta) = \\frac {p(\\mathbf{x} | \\theta)} {p(x)} = \\frac{p(x, \\theta)}{p(x)p(\\theta)} = \\frac {d^{*}(x, \\theta)} {1 - d^{*}(x, \\theta)} $$\nThen the likelihood ratio for any two hypotheses can be estimated at any point by\n$$ r(x | \\theta_0, \\theta_1) = \\frac{d^{*}(x,\\theta_0)}{d^{*}(x, \\theta_1)} $$\nToy example Setup:\nThe simulator: a function takes 1 parameter \\(\\mu\\), and return a random variable drawn from \\(\\mathcal{N}(\\mu, 1)\\) The observations \\(\\mathbf{x}\\): Observation drawn from the simulator with \\(\\mu = 2.5\\), which is unknown to the algorithm. The discriminator: A fully connected neural network. The prior of the parameters: \\(\\mathcal{N}(0, 1)\\) We want to draw samples from the posterior distribution \\(p(\\theta | \\mathbf{x})\\), where \\(x \\sim \\mathcal{N}(2.5, 1)\\).\nImplementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 import click import numpy as np import torch from matplotlib import pyplot as plt from torch import nn from torch.nn import functional as F from typing import NamedTuple np.random.seed(1) torch.manual_seed(1) def stochastic(func): def __wrapper__(*args, **kwargs): np.random.seed() rs = func(*args, **kwargs) np.random.seed(1) return rs return __wrapper__ class Layer(NamedTuple): h: int # hidden dim a: str # activation def Dense(h_i: int, h_j: int, a : str): if a == \u0026#34;tanh\u0026#34;: act = nn.Tanh() elif a == \u0026#34;sigmoid\u0026#34;: act = nn.Sigmoid() elif a == \u0026#34;relu\u0026#34;: act = nn.ReLU() else: raise NotImplementedError(a) return nn.Sequential( nn.Linear(h_i, h_j), act) def build_mlp(input_dim: int, seq: list[Layer]) -\u0026gt; nn.Module: h0, a0 = seq[0] _seq = [Dense(input_dim, h0, a0)] for j in range(1, len(seq)): h_j, a_j = seq[j] h_i, _ = seq[j - 1] _seq.append(Dense(h_i, h_j, a_j)) return nn.Sequential(*_seq) def train_step( Xpos: torch.Tensor, Xneg: torch.Tensor, d: nn.Module, opt: torch.optim.Optimizer) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Args: - Xpos: (x, theta) - Xneg: (x, theta\u0026#39;) - d: classifier Where theta/theta\u0026#39; ~ p, x ~ p(x | theta) \u0026#34;\u0026#34;\u0026#34; for i in range(32): opt.zero_grad() zpos = d(Xpos) zneg = d(Xneg) loss = F.binary_cross_entropy(zpos, torch.ones_like(zpos))\\ + F.binary_cross_entropy(zneg, torch.zeros_like(zneg)) loss.backward() opt.step() return loss.item() def train_d( p: callable, sim: callable, d: nn.Module, m: int, e: int, lr: float): \u0026#34;\u0026#34;\u0026#34; Args: - p : prior - sim: simulator (implicit p(x | theta) - d: parameterized classifier - m: batch_size - e: max epochs - lr: learning rate \u0026#34;\u0026#34;\u0026#34; opt = torch.optim.Adam(d.parameters(), lr=lr) sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt) losses = [] for b in range(e): theta = p(m) theta_prime = p(m) x = sim(theta) # expand dims everything theta = np.expand_dims(theta, -1) theta_prime = np.expand_dims(theta_prime, -1) x = np.expand_dims(x, -1) # construct training sample Xpos = np.concatenate([x, theta], -1) Xneg = np.concatenate([x, theta_prime], -1) Xpos, Xneg = torch.tensor(Xpos, dtype=torch.float),\\ torch.tensor(Xneg, dtype=torch.float) loss = train_step(Xpos, Xneg, d, opt) losses.append(loss) if b%50 == 49: sch.step(loss) return d, losses @stochastic def mcmc(lp: callable, obs: np.ndarray, d: nn.Module, n_samples: int, step_size: float): \u0026#34;\u0026#34;\u0026#34; Amortized MCMC likelihood free \u0026#34;\u0026#34;\u0026#34; # proposal distribution: q = lambda theta: np.random.normal(theta, step_size) # initialize theta theta = 0. samples = [] obs = np.expand_dims(obs, -1) for i in range(n_samples): theta_prime = q(theta) mu_theta = np.ones_like(obs) * theta mu_theta_prime = np.ones_like(obs) * theta_prime # construct input vector X = np.concatenate([obs, mu_theta], -1) Xp = np.concatenate([obs, mu_theta_prime], -1) X, Xp= torch.tensor(X, dtype=torch.float),\\ torch.tensor(Xp, dtype=torch.float) # Compute the decision function d_theta = d(X).detach().mean().numpy() d_theta_prime = d(Xp).detach().mean().numpy() r_theta = d_theta / (1 - d_theta) r_theta_prime = d_theta_prime / (1- d_theta_prime) H = r_theta_prime / r_theta H = lp(theta_prime) / lp(theta) * H H = 1 if H \u0026gt; 1 else H u = np.random.uniform() if u \u0026lt; H: # accept theta_prime samples.append(theta_prime) theta = theta_prime return samples def main( batch_size: int, max_iter: int, lr: float, n_obs: int, n_samples: int, step_size: float): # PROBLEM SETUP # -------------------------------------------------- # prior theta p = lambda m: np.random.normal(0, 1, size=m) lp = lambda x: np.exp(-0.5 * x**2)#likelihood function # simulator: unknown sim = lambda mu: np.random.normal(mu, np.ones_like(mu) * .25) # parmeterized classifier d = build_mlp( 2, [Layer(4, \u0026#39;relu\u0026#39;), Layer(2, \u0026#39;relu\u0026#39;), Layer(1, \u0026#39;sigmoid\u0026#39;)]) # TRAINING the classifier # -------------------------------------------------- d, losses = train_d(p, sim, d, m=batch_size, e=max_iter, lr=lr) # inference # -------------------------------------------------- MU = 2.5 #unknown obs = sim(np.ones(n_obs) * MU) # Posterior sample: sample p(theta | obs) samples = mcmc(lp, obs, d, n_samples, step_size) Result References The frontier of simulation-based inference Likelihood-free MCMC with Amortized Ratio Estimator I am a black-box machine, you are a black-box machine, everyone is a black-box machine as long as we don\u0026rsquo;t care enough about the person.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShamelessly copied from Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/sbi/","summary":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper.","title":"Likelihood-free MCMC with Amortized Ratio Estimator"},{"content":"Convergence in probability Definition a sequence $\\{X_n\\}$ of random variable converges in probability towards the random variable $X$ if for all $\\epsilon \u0026gt; 0$\n$$ \\lim_{n \\leftarrow \\infty} \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) = 0 $$\nConsistent estimator A statistic (singular) is any quantity computed from values in a sample which is considered fro a statistical purpose (estimating population parameter, describing sample, evaluating hypothesis).\nSufficient statistics A statistic is sufficient with respect to a statisticcal model and its associated unknown parameter if no other statistic can be\nA statistic $T(X)$ is sufficient if\n$$ I(\\theta; T(X)) = I(\\theta; X) $$\n","permalink":"http://localhost:1313/posts/miscellanous/","summary":"Convergence in probability Definition a sequence $\\{X_n\\}$ of random variable converges in probability towards the random variable $X$ if for all $\\epsilon \u0026gt; 0$\n$$ \\lim_{n \\leftarrow \\infty} \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) = 0 $$\nConsistent estimator A statistic (singular) is any quantity computed from values in a sample which is considered fro a statistical purpose (estimating population parameter, describing sample, evaluating hypothesis).\nSufficient statistics A statistic is sufficient with respect to a statisticcal model and its associated unknown parameter if no other statistic can be","title":"Miscellanous"},{"content":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$\nIn which:\n\\(y=(y_1, \u0026hellip;, y_T)\\) be a dataset of \\(T\\) observations draw from a noise density function $p_n(.)$. \\(h(u; \\theta) = 1/(1 + e^{-G(u;\\theta)})\\) \\(G(u; \\theta) = \\log p_m(u; \\theta) - \\log p_n(u)\\) For \\(p_m(.; \\theta)\\) to be a valid p.d.f, we also need to include unit integral constraint into the optimization problem, that is \\(\\int_x{p_m(x; \\theta)dx} = 1\\). However, this integral is often intractable in most cases, for example, when we use a neural network to parameterize \\(p_m(.;\\theta)\\).\nInterestingly, the paper claims that maximizing the objective function gives a valid p.d.f without placing the unit integral constraint on the optimization (Theorem 1). In this post, I\u0026rsquo;ll attempt to prove the theorem as an exercise. Note that, I made an assumption that support of $x$ and $y$ are equal (in eq.7); which mean $p_n(.)$ is nonzero whenever $p_d(.)$ is nonzero and $p_n(.)$ is zero everywhere else.\nProof of threorem I When the sample size $T$ becomes arbitrarily large, the objective function $J_T(\\theta)$ converges in probability (this is a new word for me) to $\\tilde{J}$\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) = \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{r\\big(f(x) - \\log{p_n(x)}\\big)} + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\end{aligned} \\end{equation}\nIn which \\(f(x) = \\log p_m(x; \\theta)\\) is the function approximating log-likelihood the true distribution $p_d(.)$.\nNotation \\(p_d(x)\\) true probability density function (p.d.f) of data. \\(p_n(x)\\) p.d.f of noise generating distribution. \\(r(x) = \\frac{1}{1+\\exp(-x)}\\) sigmoid function. \\(X = (x_1, \u0026hellip; x_T); x \\sim p_d(x)\\) be the dataset of T observations. \\(Y = (y_1, \u0026hellip; y_T); y \\sim p_n(y)\\) be the dataset of T artificially generated noise. \\(p_m(.; \\theta)\\) is estimation of \\(p_d(.)\\) parameterized by \\(\\theta\\). Theorem \\(\\tilde{J}\\) attains a maximum at \\(f(.) = \\log p_d(.)\\). There are no other extrema if the noise density \\(p_n(.)\\) is chosen such it is nonzero whenever \\(p_d(.)\\) is nonzero.\nProof Let \\(\\hat{f}(x)\\) be the optimal function that maximizes \\(\\tilde{J}\\), and \\(f(x)=\\hat{f}(x) + \\epsilon\\eta(x)\\).\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) \u0026amp;= K(\\epsilon) \\\\ \u0026amp;= \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{ r\\big(f(x) - \\log{p_n(x)}\\big) } + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\\\ \u0026amp;= \\frac{1}{2}\\underbrace{ \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } }_A + \\frac{1}{2} \\underbrace{\\mathbb{E}_y { \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} }}_B \\\\ \\implies \\frac{dK}{d\\epsilon} \u0026amp;= \\frac{dA}{d\\epsilon} + \\frac{dB}{d\\epsilon} \\end{aligned} \\end{equation}\nExpand the first term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} A(\\epsilon) \u0026amp;= \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } \\\\ \u0026amp; = \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\end{aligned} \\end{equation}\nTaking derivative of $A(\\epsilon)$\n\\begin{equation} \\begin{aligned} \\frac{dA}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\\\ \u0026amp; = \\int_x { p_d(x) \\big[ \\frac{1}{d\\epsilon}\\log{ r \\big( \\underbrace{ \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) }_{g(\\epsilon)} \\big) } \\big]dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{d\\log{r}}{dr} \\frac{dr}{dg} \\frac{dg}{d\\epsilon} dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{1}{r} r(1-r) \\eta(x) dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\end{aligned} \\end{equation}\nNow let\u0026rsquo;s turn our attention to the second term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} B(\\epsilon) \u0026amp;= \\mathbb{E}_y { \\log\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big] } \\\\ \u0026amp; = \\int_y { p_n(y) \\log \\big[ 1 - r \\big( \\underbrace{ \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) }_h \\big) \\big]dy } \\end{aligned} \\end{equation}\nTaking derivative of $B$ w.r.t $\\epsilon$\n\\begin{equation} \\begin{aligned} \\frac{dB}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_y{ p_n(y)\\log{ \\big[ 1 - r\\big( h(\\epsilon)\\big) \\big] }dy } \\\\ \u0026amp;= \\int_y { p_n(y) \\frac{d\\log(1-r)}{d(1-r)} \\frac{d(1-r)}{dr} \\frac{dr}{dh} \\frac{dh}{d\\epsilon} dy } \\\\ \u0026amp; = \\int_y { p_n(y) \\frac{1}{1-r} (-1) r(1-r) \\eta(y) } \\\\ \u0026amp; = -\\int_y{ p_n(y) r\\big( \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) \\big) \\eta(y) dy } \\end{aligned} \\end{equation}\nSubstitute result from eq(4) and eq(6) to eq(2), $\\frac{dK}{d\\epsilon}$ is evaluated to $0$ at $\\epsilon = 0$.\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon}\\big\\vert_{\\epsilon=0} \u0026amp;= \\frac{dA}{d\\epsilon}\\big\\vert_{\\epsilon=0} + \\frac{dB}{d\\epsilon}\\big\\vert_{\\epsilon=0} \\\\ \u0026amp;= \\int_x { p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\\\ \u0026amp; - \\int_y{ p_n(y) r \\big( \\hat{f}(y) - \\log p_n(y) \\big) \\eta(y) dy } \\\\ \u0026amp; = 0 \\end{aligned} \\end{equation}\nConsider eq. (7), if the support for $x$ and $y$ are equal, which mean we integrate $x$ and $y$ over a same region, we can change $y$ to $x$ and rewrite eq.(7) as\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon} \\big\\vert_{\\epsilon = 0} \u0026amp;= \\int_x { \\underbrace{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] }_C \\eta(x) dx } \\\\ \u0026amp; - \\int_x{ \\underbrace{ p_n(x) r \\big( \\hat{f}(x) - \\log p_n(x) \\big) }_D \\eta(x) dx } \\\\ \u0026amp; = \\int_x{(C-D)\\eta(x)dx} = 0 \\quad \\forall \\eta(x) \\end{aligned} \\end{equation}\nThe equality in eq.(8) happend if and only if \\(C=D\\). This result easily leads to \\(\\hat{f}(x) = \\log p_d(x)\\).\nReferences Noise-contrastive estimation: A new estimation principle for unnormalized statistical models ","permalink":"http://localhost:1313/posts/noise-contrastive-estimation/","summary":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$","title":"Noise constrastive estimation"},{"content":"Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.\nResources (Useful link) Video lecture Course\u0026rsquo;s homepage Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits\nLecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.\nDefinition (Empty set) A set with no elements, denoted as \\(\\emptyset\\)\nNotation\n\\(a \\in S\\): \\(a\\) is a element of \\(S\\) \\(a \\notin S\\): \\(a\\) is not a element of \\(S\\) \\(\\forall\\): for all \\(\\exists\\): there exists \\(\\implies\\): implies \\(\\iff\\): if and only if Definition\n(subset) A set \\(A\\) is a subset of \\(B\\), denoted as \\(A \\subset B\\) if: \\(a \\in A \\implies a \\in B\\) (equal) Two sets are equal if \\(A \\subset B \\land B \\subset A\\) (proper subset) \\(A \\subsetneqq B \\iff A \\subset B \\land A \\neq B\\) Set building notation\n$$ \\{ x \\in A : P(x) \\} $$\nExamples\n\\(\\mathbb{N} = \\{1, 2, 3 \\cdots\\}\\) \\(\\mathbb{Z} = \\{\\cdots,-2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q} = \\{\\frac{m}{n}: m, n \\in \\mathbb{Z}\\}\\) Real number set \\(\\mathbb{R}\\) Remark: \\(\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}\\)\nGoal: describe the real number set $\\mathbb{R}$\nDefinition (union) The union of A and B is the set $$ A \\cup B := {x: x\\in A \\lor x\\in B} $$ Definition (intersection) The intersection of A and B is the set $$ A \\cap B := {x: x\\in A \\land x \\in B} $$ Definition (different) The set different between A w.r.t B is the set $$ A\\backslash B = {x\\in A: x\\notin B} $$ Definition (complement) A complement of set A is the set $$ A^c = {x: x\\notin A} $$ Definition (disjoint) Two sets A and B are disjoint if \\(A \\cap B = \\emptyset\\).\nTheorem (De-Morgan) If A, B, C are sets then\n\\((B \\cup C)^c = B^c \\cap C^c\\) \\((B \\cap C)^c = B^c \\cup C^c\\) \\(A\\backslash (B\\cup C) = (A\\backslash B) \\cap (A\\backslash C)\\) \\(A\\backslash (B\\cap C) = (A\\backslash B) \\cup(A\\backslash C)\\) Induction A way to prove theorem about natural number.\n\\(\\mathbb{N} = \\{1, 2, 3, \\cdots \\}\\) has an ordering \\(1 \u0026lt; 2 \u0026lt; 3\u0026lt; 4 \u0026lt; \\cdots\\)\nAxiom (Well ordering of natural numbers) if \\(S\\subset \\mathbb{N}\\) and \\(S\\neq \\emptyset\\) has a least element \\(\\exists x\\in S\\) st \\(\\forall y \\in S: x\\leq y\\).\nTheorem (Induction) Let \\(P(n)\\) be a statement depending on \\(n\\in \\mathbb{N}\\). Assume:\n(Base case) \\(P(1)\\) is true (Inductive step) If \\(P(m)\\) is true, then \\(P(m+1)\\) is true. Then \\(P(n)\\) is true for all \\(n\\in \\mathbb{N}\\). Proof: Let \\(S = {n\\in\\mathbb{N}: P(n) \\text{ is not true}}\\). Want to show \\(S=\\emptyset\\)\nSuppose \\(S\\neq \\emptyset\\). By WOP.\\(\\mathbb{N}\\), \\(S\\) has a least element \\(x\\in S\\). Since \\(P(1)\\) is true, \\(1\\notin S\\), so \\(x\u0026gt;1\\).\nSince \\(x\\) is the least element in \\(S\\) \\(\\implies x-1 \\notin S\\).\nBy the definition of \\(S\\), \\(P(x-1)\\) is true, by 2) \\(\\implies P(x)\\) is true \\(\\implies x \\notin S\\).\n\\(\\therefore S = \\emptyset\\)\nUsing induction We want to prove some statement \\(\\forall n\\in\\mathbb{N}:P(n)\\) is true, we have to do two things:\nProve \\(P(1)\\). Prove \\(P(m) \\implies P(m+1)\\) Example For all \\(c\\neq 1, \\forall n\\in\\mathbb{N}\\):\n$$ 1 + c + c^2 + \\cdots +c^n=\\frac{1-c^{n+1}}{1-c} $$\nProof\n(Base case): $$ 1 + c^1 = \\frac{1-c^{1+1}}{1-c}=\\frac{(1-c)(1+c)}{1-c} = {1+c}; \\forall c\\neq1 $$\n(Inductive step) Assume: $$ 1+c+c^2+\\cdots+c^m=\\frac{1-c^{m+1}}{1-c}\\quad(*) $$ We want to show $$ 1+c+c^2+\\cdots+c^n=\\frac{1-c^{n+1}}{1-c}\\quad(**) $$ for \\(n = m+1\\). We have:\n$$ \\begin{aligned} 1+c+c^2+\\cdots+c^m+c^{m+1} \u0026amp;= \\frac{1-c^{m+1}}{1-c}+c^{m+1} \\\\ \u0026amp; = \\frac{1-c^{m+1}+c^{m+1}-c^{m+2}}{1-c}\\\\ \u0026amp; = \\frac{1-c^{(m+1)+1}}{1-c} \\end{aligned} $$ So (*) hold for \\(n=m+1\\). By induction, \\(P(n)\\) is true \\(\\forall n\\in\\mathbb{N}\\).\n","permalink":"http://localhost:1313/posts/real-analysis/","summary":"Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.\nResources (Useful link) Video lecture Course\u0026rsquo;s homepage Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits\nLecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.\nDefinition (Empty set) A set with no elements, denoted as \\(\\emptyset\\)","title":"Real Analysis - Lecture notes"}]