<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Learning on iamtu</title><link>http://localhost:1313/tags/learning/</link><description>Recent content in Learning on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 06 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/learning/index.xml" rel="self" type="application/rss+xml"/><item><title>My crash course to functional analysis</title><link>http://localhost:1313/posts/functional/</link><pubDate>Sat, 06 Sep 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/functional/</guid><description>Introduction Function analysis Definition (Metric space)
Example (Metric space)
Applications Problem sets References Introductory Functional Analysis with applications (1978) - Erwin Kreyszig</description></item><item><title>Differentiation under integral sign</title><link>http://localhost:1313/posts/diff-under-integral-sign/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/diff-under-integral-sign/</guid><description>Motivating example Evaluating following integral
$$ I = \int_0^1{\frac{1 - x^2}{\ln{x}}dx} $$
Closed-form results $$ \begin{equation} \begin{aligned} F(t) &amp;amp;= \int_0^1{\frac{1-x^t}{\ln(x)}dx} \\ \implies \frac{d}{dt}F &amp;amp;= \frac{d}{dt}\int_0^1{\frac{1-x^t}{\ln(x)}dx}\\ &amp;amp;= \int_0^1{ \frac{\partial}{\partial t} \frac{1-x^t}{\ln(x)}dx }\\ &amp;amp;= \int_0^1{ \frac{-\ln(x)x^t}{ln(x)} dx} \\ &amp;amp;= \bigg[-\frac{x^{t+1}}{t+1}\bigg]_0^1\\ &amp;amp;= -\frac{1}{t+1}\\ \implies F(t) &amp;amp;= -\ln({t+1}) \\ \implies I &amp;amp;= f(2) = -\ln3 \end{aligned} \end{equation} $$
Numerical approximation Code to produce the figure 1 2 3 4 5 6 7 8 import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.</description></item><item><title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</title><link>http://localhost:1313/posts/closed-form-kl-gaussian/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/closed-form-kl-gaussian/</guid><description>The closed form of KL divergence used in Variational Auto Encoder.
Univariate case Let
\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\) \(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\) KL divergence between \(p\) and \(q\) is defined as:
$$ \begin{aligned} \text{KL}(p\parallel q) &amp;amp;= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\ &amp;amp;= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\ &amp;amp;= \underbrace{ \int_x{p(x)\log p(x) dx}}_A - \underbrace{ \int_x{p(x)\log q(x) dx}}_B \end{aligned} $$
First quantity \(A\):
$$ \begin{aligned} A &amp;amp;= \int_x{p(x)\log p(x) dx} \\ &amp;amp;= \int_x{p(x)\big[ -\frac{1}{2}\log{2\pi\sigma_1^2 - \frac{1}{2\sigma_1^2}(x - \mu_1)^2} \big]dx}\\ &amp;amp;= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx} - \frac{1}{2\sigma_1^2} \underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\ &amp;amp;= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2} \end{aligned} $$</description></item><item><title>Miscellanous</title><link>http://localhost:1313/posts/miscellanous/</link><pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/miscellanous/</guid><description>Convergence in probability Definition a sequence $\{X_n\}$ of random variable converges in probability towards the random variable $X$ if for all $\epsilon &amp;gt; 0$
$$ \lim_{n \leftarrow \infty} \mathbb{P}(|X_n - X| &amp;gt; \epsilon) = 0 $$
Consistent estimator A statistic (singular) is any quantity computed from values in a sample which is considered fro a statistical purpose (estimating population parameter, describing sample, evaluating hypothesis).
Sufficient statistics A statistic is sufficient with respect to a statisticcal model and its associated unknown parameter if no other statistic can be</description></item><item><title>Noise constrastive estimation</title><link>http://localhost:1313/posts/noise-contrastive-estimation/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/noise-contrastive-estimation/</guid><description>TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &amp;hellip; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function
$$ J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)] $$</description></item><item><title>Real Analysis - Lecture notes</title><link>http://localhost:1313/posts/real-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/posts/real-analysis/</guid><description>Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.
Resources (Useful link) Video lecture Course&amp;rsquo;s homepage Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits
Lecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.
Definition (Empty set) A set with no elements, denoted as \(\emptyset\)</description></item></channel></rss>